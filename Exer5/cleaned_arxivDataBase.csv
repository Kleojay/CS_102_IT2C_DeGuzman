"","title","author","subject","abstract","date"
"1","deep bi-directional attention network for image super-resolution quality assessment","yixiao li, xiaoyuan yang, jun fu, guanghui yue, wei zhou","multimedia","there has emerged a growing interest in exploring efficient quality assessment algorithms for image super-resolution (sr). however, employing deep learning techniques, especially dual-branch algorithms, to automatically evaluate the visual quality of sr images remains challenging. existing sr image quality assessment (iqa) metrics based on two-stream networks lack interactions between branches. to address this, we propose a novel full-reference iqa (fr-iqa) method for sr images. specifically, producing sr images and evaluating how close the sr images are to the corresponding hr references are separate processes. based on this consideration, we construct a deep bi-directional attention network (biatten-net) that dynamically deepens visual attention to distortions in both processes, which aligns well with the human visual system (hvs). experiments on public sr quality databases demonstrate the superiority of our proposed biatten-net over state-of-the-art quality assessment methods. in addition, the visualization results and ablation study show the effectiveness of bi-directional attention.",2024-03-15
"2","action functional as early warning indicator in the space of probability measures","peng zhang, ting gao, jin guo, jinqiao duan","dynamical systems","in neuroscience, scientists have some hypotheses on the brain's critical dynamics, which means the brain may stay in a phase transition state between ordered and disordered activities. some tipping points, as a past of no return, can be critical for neural diseases. therefore, a key question is how the critical brain hypothesis relates to pathological conditions compared with normal brain functionality. action functional between two meta-stable states in stochastic dynamical systems is a good tool to study the critical transition and tipping. here we extend the conventional onsager-machlup action functional of finding the most probable transition pathway to be looking for the evolutionary transition dynamics between two meta-stable invariant sets. hence a rich theory from schrödinger bridge and optimal transport is brought in to solve this problem. furthermore, different from various early warning indicators via statistics, bifurcation theory, information theory, statistical physics, topology, graph theory, we propose a novel viewpoint of early warning indicators in the space of probability measure, which facilitates us to build indicators based on action functional. to validate our framework, we apply this methodology to a morris-lecar model introduced to analyze the transition dynamics between a meta-stable state and the homo-clinic orbit. besides, the real alzheimer's data from adni database is also investigated to study the early warning signals of transition from healthy to pre-ad states. this framework not only extends the transition path to be pathway measures between two given densities on invariant sets but also shows potential ability for early warning indicators or biomarkers in complex diseases.",2024-03-15
"3","kif: a framework for virtual integration of heterogeneous knowledge bases using wikidata","guilherme lima, marcelo machado, elton soares, sandro r. fiorini, raphael thiago, leonardo g. azevedo, viviane t. da silva, renato cerqueira","artificial intelligence","we present a knowledge integration framework (called kif) that uses wikidata as a lingua franca to integrate heterogeneous knowledge bases. these can be triplestores, relational databases, csv files, etc., which may or may not use the wikidata dialect of rdf. kif leverages wikidata's data model and vocabulary plus user-defined mappings to expose a unified view of the integrated bases while keeping track of the context and provenance of their statements. the result is a virtual knowledge base which behaves like an ""extended wikidata"" and which can be queried either through an efficient filter interface or using sparql. we present the design and implementation of kif, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving wikidata, pubchem, and ibm circa), and present experimental results on the performance and overhead of kif.",2024-03-15
"4","h$α$ time delays of agns from the zwicky transcient facility broadband photometry","qinchun ma, yuhan wen, xue-bing wu, huapeng gu, yuming fu","astrophysics of galaxies","in our previous work on broadband photometric reverberation mapping (prm), we proposed the iccf-cut process to obtain the time lags of h$\alpha$ emission line from two broadband lightcurves via subtracting the continuum emission from the line band. extending the work, we enlarge our sample to the zwicky transient facility (ztf) database. we adopt two criteria to select 123 type 1 agns with sufficient variability and smooth lightcurves from 3537 agns at $z<0.09$ with more than 100 epoch observations in the $g$ and $r$ bands from the ztf database. we calculate the h$\alpha$ time lags for 23 of them which have previous spectroscopic reverberation mapping (srm) results using iccf-cut, javelin and $\chi ^2$ methods. our obtained h$\alpha$ time lags are slightly larger than the h$\beta$ time lags, which is consistent with the previous srm results and the theoretical model of the agn broad line region. the comparisons between srm and prm lag distributions and between the subtracted emission line lightcurves indicate that after selecting agns with the two criteria, combining the iccf-cut, javelin and $\chi^2$ methods provides an efficient way to get the reliable h$\alpha$ lags from the broadband prm. such techniques can be used to estimate the black hole masses of a large sample of agns in the large multi-epoch photometric sky surveys such as the legacy survey of space and time (lsst) and the survey from the wide field survey telescope (wfst) in the near future.",2024-03-15
"5","kp-red: exploiting semantic keypoints for joint 3d shape retrieval and deformation","ruida zhang, chenyangguang zhang, yan di, fabian manhardt, xingyu liu, federico tombari, xiangyang ji","computer vision and pattern recognition","in this paper, we present kp-red, a unified keypoint-driven retrieval and deformation framework that takes object scans as input and jointly retrieves and deforms the most geometrically similar cad models from a pre-processed database to tightly match the target. unlike existing dense matching based methods that typically struggle with noisy partial scans, we propose to leverage category-consistent sparse keypoints to naturally handle both full and partial object scans. specifically, we first employ a lightweight retrieval module to establish a keypoint-based embedding space, measuring the similarity among objects by dynamically aggregating deformation-aware local-global features around extracted keypoints. objects that are close in the embedding space are considered similar in geometry. then we introduce the neural cage-based deformation module that estimates the influence vector of each keypoint upon cage vertices inside its local support region to control the deformation of the retrieved shape. extensive experiments on the synthetic dataset partnet and the real-world dataset scan2cad demonstrate that kp-red surpasses existing state-of-the-art approaches by a large margin. codes and trained models will be released in this https url.",2024-03-15
"6","accelerating regular path queries over graph database with processing-in-memory","ruoyan ma, shengan zheng, guifeng wang, jin pu, yifan hua, wentao wang, linpeng huang","databases","regular path queries (rpqs) in graph databases are bottlenecked by the memory wall. emerging processing-in-memory (pim) technologies offer a promising solution to dispatch and execute path matching tasks in parallel within pim modules. we present moctopus, a pim-based data management system for graph databases that supports efficient batch rpqs and graph updates. moctopus employs a pim-friendly dynamic graph partitioning algorithm, which tackles graph skewness and preserves graph locality with low overhead for rpq processing. moctopus enables efficient graph update by amortizing the host cpu's update overhead to pim modules. evaluation of moctopus demonstrates superiority over the state-of-the-art traditional graph database.",2024-03-15
"7","pet-sql: a prompt-enhanced two-stage text-to-sql framework with cross-consistency","zhishuai li, xiang wang, jingjing zhao, sun yang, guoqing du, xiaoru hu, bin zhang, yuxiao ye, ziyue li, rui zhao, hangyu mao","computation and language","recent advancements in text-to-sql (text2sql) emphasize stimulating the large language models (llm) on in-context learning, achieving significant results. nevertheless, they face challenges when dealing with verbose database information and complex user intentions. this paper presents a two-stage framework to enhance the performance of current llm-based natural language to sql systems. we first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct llms in generating sql queries. then, in the first stage, question-sql pairs are retrieved as few-shot demonstrations, prompting the llm to generate a preliminary sql (presql). after that, the mentioned entities in presql are parsed to conduct schema linking, which can significantly compact the useful information. in the second stage, with the linked schema, we simplify the prompt's schema information and instruct the llm to produce the final sql. finally, as the post-refinement module, we propose using cross-consistency across different llms rather than self-consistency within a particular llm. our methods achieve new sota results on the spider benchmark, with an execution accuracy of 87.6%.",2024-03-13
"8","open-universe indoor scene generation using llm program synthesis and uncurated object databases","rio aguina-kang, maxim gumin, do heon han, stewart morris, seung jean yoo, aditya ganeshan, r. kenny jones, qiuhong anna wei, kailiang fu, daniel ritchie","computer vision and pattern recognition","we present a system for generating indoor scenes in response to text prompts. the prompts are not limited to a fixed vocabulary of scene descriptions, and the objects in generated scenes are not restricted to a fixed set of object categories -- we call this setting indoor scene generation. unlike most prior work on indoor scene generation, our system does not require a large training dataset of existing 3d scenes. instead, it leverages the world knowledge encoded in pre-trained large language models (llms) to synthesize programs in a domain-specific layout language that describe objects and spatial relations between them. executing such a program produces a specification of a constraint satisfaction problem, which the system solves using a gradient-based optimization scheme to produce object positions and orientations. to produce object geometry, the system retrieves 3d meshes from a database. unlike prior work which uses databases of category-annotated, mutually-aligned meshes, we develop a pipeline using vision-language models (vlms) to retrieve meshes from massive databases of un-annotated, inconsistently-aligned meshes. experimental evaluations show that our system outperforms generative models trained on 3d data for traditional, closed-universe scene generation tasks; it also outperforms a recent llm-based layout generation method on open-universe scene generation.",2024-02-05
"9","localization in digital twin mimo networks: a case for massive fingerprinting","joão morais, ahmed alkhateeb","information theory","localization in outdoor wireless systems typically requires transmitting specific reference signals to estimate distance (trilateration methods) or angle (triangulation methods). these cause overhead on communication, need a los link to work well, and require multiple base stations, often imposing synchronization or specific hardware requirements. fingerprinting has none of these drawbacks, but building its database requires high human effort to collect real-world measurements. for a long time, this issue limited the size of databases and thus their performance. this work proposes significantly reducing human effort in building fingerprinting databases by populating them with \textit{digital twin rf maps}. these rf maps are built from ray-tracing simulations on a digital replica of the environment across several frequency bands and beamforming configurations. online user fingerprints are then matched against this spatial database. the approach was evaluated with practical simulations using realistic propagation models and user measurements. our experiments show sub-meter localization errors on a nlos location 95\% of the time using sensible user measurement report sizes. results highlight the promising potential of the proposed digital twin approach for ubiquitous wide-area 6g localization.",2024-03-14
"10","iterative forgetting: online data stream regression using database-inspired adaptive granulation","niket kathiriya, hossein haeri, cindy chen, kshitij jerath","machine learning","many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making. such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to. there exists a need to create novel data stream regression methods that can handle these scenarios. we present a database-inspired datastream regression model that (a) uses inspiration from r*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions. the r*-tree-inspired approach also makes the algorithm amenable to integration with database systems. our experiments demonstrate that the ability of this method to discard data produces a significant order-of-magnitude improvement in latency and training time when evaluated against the most accurate state-of-the-art algorithms, while the r*-tree-inspired granulation technique provides competitively accurate predictions",2024-03-14
"11","hex: high-pressure elemental xstals, a complete database","federico giannessi, simone di cataldo, santanu saha, lilia boeri","materials science","this paper introduces the hex (high-pressure elemental xstals) database, a complete database of the ground-state crystal structures of the first 57 elements of the periodic table, from h to la, at 0, 100, 200 and 300 gpa. hex aims to provide a unified reference for high-pressure research, by compiling all available experimental information on elements at high pressure, and complementing it with the results of accurate evolutionary crystal structure prediction runs based on density functional theory. besides offering a much-needed reference, our work also serves as a benchmark of the accuracy of current ab-initio methods for crystal structure prediction. we find that, in 98 % of the cases in which experimental information is available, ab-initio crystal structure prediction yields structures which either coincide or are degenerate in enthalpy to within 300 k with experimental ones. the main manuscript contains synthetic tables and figures, while the crystallographic information file (cif) for all structures will be available on a figshare online repository when the paper will be published.",2024-03-14
"12","algorithmic syntactic causal identification","dhurim cakiqi, max a. little","artificial intelligence","causal identification in causal bayes nets (cbns) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. however, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on cbns. however, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. we show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. in this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. this allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general id algorithm through fixing. our description is given entirely in terms of the non-parametric admg structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. we use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.",2024-03-14
"13","the nerfect match: exploring nerf features for visual localization","qunjie zhou, maxim maximov, or litany, laura leal-taixé","computer vision and pattern recognition","in this work, we propose the use of neural radiance fields (nerf) as a scene representation for visual localization. recently, nerf has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. we extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of nerf's internal features in establishing precise 2d-3d matches for localization. to this end, we conduct a comprehensive examination of nerf's implicit knowledge, acquired through view synthesis, for matching under various conditions. this includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. significantly, we introduce nerfmatch, an advanced 2d-3d matching function that capitalizes on the internal knowledge of nerf learned via view synthesis. our evaluation of nerfmatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on cambridge landmarks.",2024-03-14
"14","impact of synthetic images on morphing attack detection using a siamese network","juan tapia, christoph busch","computer vision and pattern recognition","this paper evaluated the impact of synthetic images on morphing attack detection (mad) using a siamese network with a semi-hard-loss function. intra and cross-dataset evaluations were performed to measure synthetic image generalisation capabilities using a cross-dataset for evaluation. three different pre-trained networks were used as feature extractors from traditional mobilenetv2, mobilenetv3 and efficientnetb0. our results show that mad trained on efficientnetb0 from feret, frgcv2, and frll can reach a lower error rate in comparison with sota. conversely, worse performances were reached when the system was trained only with synthetic images. a mixed approach (synthetic + digital) database may help to improve mad and reduce the error rate. this fact shows that we still need to keep going with our efforts to include synthetic images in the training process.",2024-03-14
"15","retrieval augmented text-to-sql generation for epidemiological question answering using electronic health records","angelo ziletti, leonardo d'ambrosi","computation and language","electronic health records (ehr) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex sql queries. here, we introduce an end-to-end methodology that combines text-to-sql generation with retrieval augmented generation (rag) to answer epidemiological questions using ehr and claims data. we show that our approach, which integrates a medical coding step into the text-to-sql process, significantly improves the performance over simple prompting. our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, rag offers a promising direction for improving their capabilities, as shown in a realistic industry setting.",2024-03-14
"16","ai on ai: exploring the utility of gpt as an expert annotator of ai publications","autumn toney-wails, christian schoeberl, james dunham","computation and language","identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts. resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (ai), which spans emerging topics and technologies. we address these challenges by inferring a functional definition of ai research from existing expert labels, and then evaluating state-of-the-art chatbot models on the task of expert data annotation. using the arxiv publication database as ground-truth, we experiment with prompt engineering for gpt chatbot models to identify an alternative, automated expert annotation pipeline that assigns ai labels with 94% accuracy. for comparison, we fine-tune specter, a transformer language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than gpt) on classifying ai publications. our results indicate that with effective prompt engineering, chatbots can be used as reliable data annotators even where subject-area expertise is required. to evaluate the utility of chatbot-annotated datasets on downstream classification tasks, we train a new classifier on gpt-labeled data and compare its performance to the arxiv-trained model. the classifier trained on gpt-labeled data outperforms the arxiv-trained model by nine percentage points, achieving 82% accuracy.",2024-03-14
"17","digitization of astronomical photographic plate of china and astrometric measurement of single-exposure plates","zheng-jun shang, yong yu, liang-liang wang, mei-ting yang, jing yang, shi-yin shen, min liu, quan-feng xu, chen-zhou cui, dong-wei fan, zheng-hong tang, jian-hai zhao","instrumentation and methods for astrophysics","from the mid-19th century to the end of the 20th century, photographic plates served as the primary detectors for astronomical observations. astronomical photographic observations in china began in 1901, and over a century, a total of approximately 30,000 astronomical photographic plates have been captured. these historical plates play an irreplaceable role in conducting long-term, time-domain astronomical research. to preserve and explore these valuable original astronomical observational data, shanghai astronomical observatory has organized the transportation of plates taken at night from various stations across the country to the sheshan plate archive for centralized preservation. for the first time, plate information statistics was performed. on this basis, the plates were cleaned and digitally scanned, and finally digitized images were acquired for 29,314 plates. in this study, using gaia dr2 as the reference star catalog, astrometric processing has been carried out successfully on 15,696 single-exposure plates, including object extraction, stellar identification, and plate model computation. as a result, for long focal length telescopes, such as the 40cm double-tube refractor telescope and the 1.56m reflector telescope at the shanghai astronomical observatory and the 1m reflector telescope at the yunnan astronomical observatory, the astrometric accuracy obtained for their plates is approximately 0.1"" to 0.3"". the distribution of astrometric accuracy for medium and short focal length telescopes ranges from 0.3"" to 1.0"". the relevant data of this batch of plates, including digitized images and stellar catalog of the plates are archived and released by the national astronomical data center. users can access and download plate data based on keywords such as station, telescope, observation year, and observed celestial coordinates.",2024-03-14
"18","using deep learning for morphological classification in pigs with a focus on sanitary monitoring","eduardo bedin, junior silva souza, gabriel toshio hirokawa higa, alexandre pereira, charles kiefer, newton loebens, hemerson pistori","computer vision and pattern recognition","the aim of this paper is to evaluate the use of d-cnn (deep convolutional neural networks) algorithms to classify pig body conditions in normal or not normal conditions, with a focus on characteristics that are observed in sanitary monitoring, and were used six different algorithms to do this task. the study focused on five pig characteristics, being these caudophagy, ear hematoma, scratches on the body, redness, and natural stains (brown or black). the results of the study showed that d-cnn was effective in classifying deviations in pig body morphologies related to skin characteristics. the evaluation was conducted by analyzing the performance metrics precision, recall, and f-score, as well as the statistical analyses anova and the scott-knott test. the contribution of this article is characterized by the proposal of using d-cnn networks for morphological classification in pigs, with a focus on characteristics identified in sanitary monitoring. among the best results, the average precision metric of 80.6\% to classify caudophagy was achieved for the inceptionresnetv2 network, indicating the potential use of this technology for the proposed task. additionally, a new image database was created, containing various pig's distinct body characteristics, which can serve as data for future research.",2024-03-13
"19","measuring the bioeconomy economically: exploring the connections between concepts, methods, data, indicators and their limitations","sebastián leavy, gabriela allegretti, elen presotto, marco antonio montoya, edson talamini","general economics","despite its relevance, measuring the contributions of the bioeconomy to national economies remains an arduous task that faces limitations. part of the difficulty is associated with the lack of a clear and widely accepted concept of the bioeconomy and moves on to the connections between methods, data and indicators. the present study aims to define the concepts of bioeconomy and to explore the connections between concepts, methods, data and indicators when measuring the bioeconomy economically, and the limitations involved in this process. the bioeconomy concepts were defined based on a literature review and a content analysis of 84 documents selected through snowballing procedures to find articles measuring 'how big is the bioeconomy?'. the content of the 84 documents was uploaded to the qda miner software and coded according to the bioeconomy concept, the methods or models used, the data sources accessed, the indicators calculated, and the limitations reported by the authors. the results of the occurrence and co-occurrence of the codes were extracted and analyzed statistically, indicating that the measurement of bioeconomy (i) need recognize and pursue the proposed concept of holistic bioeconomy; (ii) rarely considered aspects of holistic bioeconomy (3.5%); (iii) is primarily based on the concept of biomass-based bioeconomy (bmbb) (94%); (iv) the association with the concept of biosphere (bsbb) appeared in 26% of the studies; (v) the biotech-based bioeconomy (btbb) was the least frequent (1.2%); (vi) there is a diversity of methods and models, but the most common are those traditionally used to measure macroeconomic activities, especially input-output models; (vii) depending on the prevailing methods, the data comes from various official statistical databases, such as national accounts and economic activity classification systems;...",2024-03-13
"20","stacking-based deep neural network for player scouting in football 1","simon lacan (imt nord europe)","machine learning","datascouting is one of the most known data applications in professional sport, and specifically football. its objective is to analyze huge database of players in order to detect high potentials that can be then individually considered by human scouts. in this paper, we propose a stacking-based deep learning model to detect high potential football players. applied on open-source database, our model obtains significantly better results that classical statistical methods.",2024-03-13
"21","ilciter: evidence-grounded interpretable local citation recommendation","sayar ghosh roy, jiawei han","information retrieval","existing machine learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. to alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ilciter, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. unlike past formulations that simply output recommendations, ilciter retrieves ranked lists of evidence span and recommended paper pairs. secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers. in contrast, ilciter relies solely on distant supervision from a dynamic evidence database and pre-trained transformer-based language models without any model training. we contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans.",2024-03-13
"22","evaluation and comparison of covariate balance metrics in studies with time-dependent confounding","david adenyo, jason r. guertin, bernard candas, caroline sirois, denis talbot","methodology","marginal structural models have been increasingly used by analysts in recent years to account for confounding bias in studies with time-varying treatments. the parameters of these models are often estimated using inverse probability of treatment weighting. to ensure that the estimated weights adequately control confounding, it is possible to check for residual imbalance between treatment groups in the weighted data. several balance metrics have been developed and compared in the cross-sectional case but have not yet been evaluated and compared in longitudinal studies with time-varying treatment. we have first extended the definition of several balance metrics to the case of a time-varying treatment, with or without censoring. we then compared the performance of these balance metrics in a simulation study by assessing the strength of the association between their estimated level of imbalance and bias. we found that the mahalanobis balance performed best.finally, the method was illustrated for estimating the cumulative effect of statins exposure over one year on the risk of cardiovascular disease or death in people aged 65 and over in population-wide administrative data. this illustration confirms the feasibility of employing our proposed metrics in large databases with multiple time-points.",2024-03-13
"23","metallicities for more than 10 million stars derived from gaia bp/rp spectra","t. xylakis-dornbusch, n. christlieb, t.t. hansen, t.nordlander, k. b. webber, j. marshall","solar and stellar astrophysics","context. the third gaia data release, which includes bp/rp spectra for 219 million sources, has opened a new window in the exploration of the chemical history and evolution of the milky way. the wealth of information encapsulated in these data is far greater than their low resolving power (r=50) at first glance would suggest, as shown in many studies. we zero in on the use of this data for the purpose of the detection of ''new'' metal-poor stars, which are hard to find yet essential for understanding - among other - several aspects of the origin of the galaxy, star formation and the creation of the elements. aims. we strive to refine a metal-poor candidate selection method which was developed with simulated gaia bp/rp spectra, with an ultimate objective of providing the community with both a recipe to select stars for medium/high resolution observations and a catalogue of stellar metallicities. methods. we used a datased comprised of galah dr3 and saga database stars in order to verify and adjust to real world data our selection method. for that purpose, we used dereddening as a mean to tackle the issue of extinction, and then we applied our fine-tuned method to select metal-poor candidates, which we thereafter observed and analysed. results. we were able to infer metallicities for galah dr3 and saga stars - with color excesses up to e(b-v)<1.5 - with an uncertainty of 0.36 dex, which is good enough for the purpose of identifying new metal-poor stars. further, we selected 26 metal-poor candidates - via our method - for observations. as spectral analysis showed, 100% of them had [fe/h]<-2.0, 57% had [fe/h]<-2.5 and 8% had [fe/h]<-3.0. we inferred metallicities for these stars with an uncertainty of 0.31 dex, as was proven when comparing to the spectroscopic [fe/h]. finally, we assembled a catalogue of metallicities for 10 861 062 stars.",2024-03-13
"24","the development and performance of a machine learning based mobile platform for visually determining the etiology of penile pathology","lao-tzu allan-blitz, sithira ambepitiya, raghavendra tirupathi, jeffrey d. klausner, yudara kularathne","image and video processing","machine-learning algorithms can facilitate low-cost, user-guided visual diagnostic platforms for addressing disparities in access to sexual health services. we developed a clinical image dataset using original and augmented images for five penile diseases: herpes eruption, syphilitic chancres, penile candidiasis, penile cancer, and genital warts. we used a u-net architecture model for semantic pixel segmentation into background or subject image, the inception-resnet version 2 neural architecture to classify each pixel as diseased or non-diseased, and a salience map using gradcam++. we trained the model on a random 91% sample of the image database using 150 epochs per image, and evaluated the model on the remaining 9% of images, assessing recall (or sensitivity), precision, specificity, and f1-score (accuracy). of the 239 images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%) were of hsv infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of penile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were of non-diseased penises. the overall accuracy of the model for correctly classifying the diseased image was 0.944. between july 1st and october 1st 2023, there were 2,640 unique users of the mobile platform. among a random sample of submissions (n=437), 271 (62.0%) were from the united states, 64 (14.6%) from singapore, 41 (9.4%) from candia, 40 (9.2%) from the united kingdom, and 21 (4.8%) from vietnam. the majority (n=277 [63.4%]) were between 18 and 30 years old. we report on the development of a machine-learning model for classifying five penile diseases, which demonstrated excellent performance on a validation dataset. that model is currently in use globally and has the potential to improve access to diagnostic services for penile diseases.",2024-03-13
"25","learning to describe for predicting zero-shot drug-drug interactions","fangqi zhu, yongqi zhang, lei chen, bing qin, ruifeng xu","computation and language","adverse drug-drug interactions~(ddis) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. as the development of new drugs continues, the potential for unknown adverse effects resulting from ddis becomes a growing concern. traditional computational methods for ddi prediction may fail to capture interactions for new drugs due to the lack of knowledge. in this paper, we introduce a new problem setup as zero-shot ddi prediction that deals with the case of new drugs. leveraging textual information from online databases like drugbank and pubchem, we propose an innovative approach textddi with a language model-based ddi predictor and a reinforcement learning~(rl)-based information selector, enabling the selection of concise and pertinent text for accurate ddi prediction on new drugs. empirical results show the benefits of the proposed approach on several settings including zero-shot and few-shot ddi prediction, and the selected texts are semantically relevant. our code and data are available at \url{this https url}.",2024-03-13
"26","translating between sql dialects for cloud migration","ran zmigrod, salwa alamir, xiaomo liu","databases","migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. a crucial component of such cloud migrations is the transition of databases to be hosted online. in this work, we consider the difficulties of this migration for sql databases. while sql is one of the prominent methods for storing database procedures, there are a plethora of different sql dialects (e.g., mysql, postgres, etc.) which can complicate migrations when the on-premise sql dialect differs to the dialect hosted on the cloud. tools exist by common cloud provides such as aws and azure to aid in translating between dialects in order to mitigate the majority of the difficulties. however, these tools do not successfully translate $100\%$ of the code. consequently, software engineers must manually convert the remainder of the untranslated database. for large organizations, this task quickly becomes intractable and so more innovative solutions are required. we consider this challenge a novel yet vital industrial research problem for any large corporation that is considering cloud migrations. furthermore, we introduce potential avenues of research to tackle this challenge that have yielded promising preliminary results.",2024-03-13
"27","versatile defense against adversarial attacks on image recognition","haibo zhang, zhihua yao, kouichi sakurai","computer vision and pattern recognition","adversarial attacks present a significant security risk to image recognition tasks. defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks. another important factor is the resources involved in terms of time and cost for training defense models and updating the model database. training many models that are specific to each type of attack can be time-consuming and expensive. ideally, we should be able to train one single model that can handle a wide range of attacks. it appears that a defense method based on image-to-image translation may be capable of this. the proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown adversarial attacks. the trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies. when facing the pgd attack and the mi-fgsm attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks. the robustness check also shows that our versatile defense model performs stably regardless with the attack strength.",2024-03-13
"28","physics-informed generative model for drug-like molecule conformers","david c. williams, neil inala","biomolecules","we present a diffusion-based, generative model for conformer generation. our model is focused on the reproduction of bonded structure and is constructed from the associated terms traditionally found in classical force fields to ensure a physically relevant representation. techniques in deep learning are used to infer atom typing and geometric parameters from a training set. conformer sampling is achieved by taking advantage of recent advancements in diffusion-based generation. by training on large, synthetic data sets of diverse, drug-like molecules optimized with the semiempirical gfn2-xtb method, high accuracy is achieved for bonded parameters, exceeding that of conventional, knowledge-based methods. results are also compared to experimental structures from the protein databank (pdb) and cambridge structural database (csd).",2024-02-29
"29","on hrtf notch frequency prediction using anthropometric features and neural networks","lior arbel, ishwarya ananthabhotla, zamir ben-hur, david lou alon, boaz rafaely","audio and speech processing","high fidelity spatial audio often performs better when produced using a personalized head-related transfer function (hrtf). however, the direct acquisition of hrtfs is cumbersome and requires specialized equipment. thus, many personalization methods estimate hrtf features from easily obtained anthropometric features of the pinna, head, and torso. the first hrtf notch frequency (n1) is known to be a dominant feature in elevation localization, and thus a useful feature for hrtf personalization. this paper describes the prediction of n1 frequency from pinna anthropometry using a neural model. prediction is performed separately on three databases, both simulated and measured, and then by domain mixing in-between the databases. the model successfully predicts n1 frequency for individual databases and by domain mixing between some databases. prediction errors are better or comparable to those previously reported, showing significant improvement when acquired over a large database and with a larger output range.",2024-03-12
"30","physics-transfer learning for material strength screening","yingjie zhao, zian zhang, zhiping xu","materials science","the strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance. peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow. the determination of peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips. material screening by strength via the peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases. in this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic simulations and then predict the peierls stress from chemically accurate density functional theory-based calculations of material parameters. notably, the strengths of single-crystalline metals can be predicted from a few single-point calculations for the deformed lattice and on the {\gamma} surface, allowing efficient, high-throughput screening for material discovery. uncertainty quantification is carried out to assess the accuracy of models and sources of errors, showing reduced physical and system uncertainties in the predictions by elevating the fidelity of training models. this physics-transfer framework can be generalized to other problems facing the accuracy-performance dilemma, by harnessing the hierarchy of physics in the multiscale models of materials science.",2024-03-12
"31","hybrid data management architecture for present quantum computing","markus zajac, uta störl","emerging technologies","quantum computers promise polynomial or exponential speed-up in solving certain problems compared to classical computers. however, in practical use, there are currently a number of fundamental technical challenges. one of them concerns the loading of data into quantum computers, since they cannot access common databases. in this vision paper, we develop a hybrid data management architecture in which databases can serve as data sources for quantum algorithms. to test the architecture, we perform experiments in which we assign data points stored in a database to clusters. for cluster assignment, a quantum algorithm processes this data by determining the distances between data points and cluster centroids.",2024-03-12
"32","generalised graph grammars for natural language processing","oliver robert fox, giacomo bergami","databases","this seminal paper proposes a new query language for graph matching and rewriting overcoming {the declarative} limitation of cypher while outperforming {neo4j} on graph matching and rewriting by at least one order of magnitude. we exploited columnar databases (knobab) to represent graphs using the generalised semistructured model.",2024-03-12
"33","dalsa: domain adaptation for supervised learning from sparsely annotated mr images","michael götz, christian weber, franciszek binczyk, joanna polanska, rafal tarnawski, barbara bobek-billewicz, ullrich köthe, jens kleesiek, bram stieltjes, klaus h. maier-hein","image and video processing","we propose a new method that employs transfer learning techniques to effectively correct sampling selection errors introduced by sparse annotations during supervised learning for automated tumor segmentation. the practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup. the comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone. the proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs domain adaptation techniques for effectively correcting sampling selection errors introduced by the sparse sampling. the new approach is validated on labeled, multi-modal mr images of 19 patients with malignant gliomas and by comparative analysis on the brats 2013 challenge data sets. compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy. this dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification.",2024-03-12
"34","input data adaptive learning (idal) for sub-acute ischemic stroke lesion segmentation","michael götz, christian weber, christoph kolb, klaus maier-hein","image and video processing","in machine learning larger databases are usually associated with higher classification accuracy due to better generalization. this generalization may lead to non-optimal classifiers in some medical applications with highly variable expressions of pathologies. this paper presents a method for learning from a large training base by adaptively selecting optimal training samples for given input data. in this way heterogeneous databases are supported two-fold. first, by being able to deal with sparsely annotated data allows a quick inclusion of new data set and second, by training an input-dependent classifier. the proposed approach is evaluated using the siss challenge. the proposed algorithm leads to a significant improvement of the classification accuracy.",2024-03-12
"35","advancements in continuous glucose monitoring: integrating deep learning and ecg signal","mohammadreza hosseinzadehketilateh, banafsheh adami, nima karimian","signal processing","this paper presents a novel approach to noninvasive hyperglycemia monitoring utilizing electrocardiograms (ecg) from an extensive database comprising 1119 subjects. previous research on hyperglycemia or glucose detection using ecg has been constrained by challenges related to generalization and scalability, primarily due to using all subjects' ecg in training without considering unseen subjects as a critical factor for developing methods with effective generalization. we designed a deep neural network model capable of identifying significant features across various spatial locations and examining the interdependencies among different features within each convolutional layer. to expedite processing speed, we segment the ecg of each user to isolate one heartbeat or one cycle of the ecg. our model was trained using data from 727 subjects, while 168 were used for validation. the testing phase involved 224 unseen subjects, with a dataset consisting of 9,000 segments. the result indicates that the proposed algorithm effectively detects hyperglycemia with a 91.60% area under the curve (auc), 81.05% sensitivity, and 85.54% specificity.",2024-03-12
"36","tcam-ssd: a framework for search-based computing in solid-state drives","ryan wong, nikita kim, kevin higgs, sapan agarwal, engin ipek, saugata ghose, ben feinberg","hardware architecture","as the amount of data produced in society continues to grow at an exponential rate, modern applications are incurring significant performance and energy penalties due to high data movement between the cpu and memory/storage. while processing in main memory can alleviate these penalties, it is becoming increasingly difficult to keep large datasets entirely in main memory. this has led to a recent push for in-storage computation, where processing is performed inside the storage device. we propose tcam-ssd, a new framework for search-based computation inside the nand flash memory arrays of a conventional solid-state drive (ssd), which requires lightweight modifications to only the array periphery and firmware. tcam-ssd introduces a search manager and link table, which can logically partition the nand flash memory's contents into search-enabled regions and standard storage regions. together, these light firmware changes enable tcam-ssd to seamlessly handle block i/o operations, in addition to new search operations, thereby reducing end-to-end execution time and total data movement. we provide an nvme-compatible interface that provides programmers with the ability to dynamically allocate data on and make use of tcam-ssd, allowing the system to be leveraged by a wide variety of applications. we evaluate three example use cases of tcam-ssd to demonstrate its benefits. for transactional databases, tcam-ssd can mitigate the performance penalties for applications with large datasets, achieving a 60.9% speedup over a conventional system that retrieves data from the ssd and computes using the cpu. for database analytics, tcam-ssd provides an average speedup of 17.7x over a conventional system for a collection of analytical queries. for graph analytics, we combine tcam-ssd's associative search with a sparse data structure, speeding up graph computing for larger-than-memory datasets by 14.5%.",2024-03-11
"37","deep adaptative spectral zoom for improved remote heart rate estimation","joaquim comas, adria ruiz, federico sukno","computer vision and pattern recognition","recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy. however, these improvements primarily focus on recovering the rppg signal, overlooking the implicit challenges of estimating the heart rate (hr) from the derived signal. while many methods employ the fast fourier transform (fft) for hr estimation, the performance of the fft is inherently affected by a limited frequency resolution. in contrast, the chirp-z transform (czt), a generalization form of fft, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation. this paper presents the advantages of employing the czt for remote hr estimation and introduces a novel data-driven adaptive czt estimator. the objective of our proposed model is to tailor the czt to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of hr from the rppg signal without compromising generalization across diverse datasets. this is achieved through a sparse matrix optimization (smo). we validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets ucla-rppg, pure, and ubfc-rppg employing both intra- and cross-database performance metrics. the results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rppg method.",2024-03-11
"38","cood: combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification","l. e. hogeweg, r. gangireddy, d. brunink, v. j. kalkman, l. cornelissen, j.w. kamminga","computer vision and pattern recognition","high-performing out-of-distribution (ood) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. in this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. we propose a framework for combining individual ood measures into one combined ood (cood) measure using a supervised model. the individual measures are several existing state-of-the-art measures and several novel ood measures developed with novel class detection and hierarchical class structure in mind. cood was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. we show that cood outperforms individual, including state-of-the-art, ood measures by a large margin in terms of tpr@1% fpr in the majority of experiments, e.g., improving detecting imagenet images (ood) from 54.3% to 85.4% for the inaturalist 2018 dataset. shap (feature contribution) analysis shows that different individual ood measures are essential for various tasks, indicating that multiple ood measures and combinations are needed to generalize. additionally, we show that explicitly considering id images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing ood detection methods and for practical applicability. the framework can easily be extended or adapted to other tasks and media modalities.",2024-03-11
"39","concurrent speaker detection: a multi-microphone transformer-based approach","amit eliav, sharon gannot","audio and speech processing","we present a deep-learning approach for the task of concurrent speaker detection (csd) using a modified transformer model. our model is designed to handle multi-microphone data but can also work in the single-microphone case. the method can classify audio segments into one of three classes: 1) no speech activity (noise only), 2) only a single speaker is active, and 3) more than one speaker is active. we incorporate a cost-sensitive (cs) loss and a confidence calibration to the training procedure. the approach is evaluated using three real-world databases: ami, alimeeting, and chime 5, demonstrating an improvement over existing approaches.",2024-03-11
"40","born to run, programmed to play: mapping the extended reality exergames landscape","sukran karaosmanoglu, sebastian cmentowski, lennart e. nacke, frank steinicke","human-computer interaction","many people struggle to exercise regularly, raising the risk of serious health-related issues. extended reality (xr) exergames address these hurdles by combining physical exercises with enjoyable, immersive gameplay. while a growing body of research explores xr exergames, no previous review has structured this rapidly expanding research landscape. we conducted a scoping review of the current state of xr exergame research to (i) provide a structured overview, (ii) highlight trends, and (iii) uncover knowledge gaps. after identifying 1318 papers in human-computer interaction and medical databases, we ultimately included 186 papers in our analysis. we provide a quantitative and qualitative summary of xr exergame research, showing current trends and potential future considerations. finally, we provide a taxonomy of xr exergames to help future design and methodological investigation and reporting.",2024-03-11
"41","zero-shot ecg classification with multimodal learning and test-time clinical knowledge enhancement","che liu, zhongwei wan, cheng ouyang, anand shah, wenjia bai, rossella arcucci","signal processing","electrocardiograms (ecgs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. while ecg self-supervised learning (essl) methods show promise in representation learning from unannotated ecg data, they often overlook the clinical knowledge that can be found in reports. this oversight and the requirement for annotated samples for downstream tasks limit essl's versatility. in this work, we address these issues with the multimodal ecg representation learning (merl}) framework. through multimodal learning on ecg records and associated reports, merl is capable of performing zero-shot ecg classification with text prompts, eliminating the need for training data in downstream tasks. at test time, we propose the clinical knowledge enhanced prompt engineering (ckepe) approach, which uses large language models (llms) to exploit external expert-verified clinical knowledge databases, generating more descriptive prompts and reducing hallucinations in llm-generated content to boost zero-shot classification. based on merl, we perform the first benchmark across six public ecg datasets, showing the superior performance of merl compared against essl methods. notably, merl achieves an average auc score of 75.2% in zero-shot classification (without training data), 3.2% higher than linear probed essl methods with 10\% annotated training data, averaged across all six datasets.",2024-03-11
"42","a survey of learned indexes for the multi-dimensional space","abdullah al-mamun, hao wu, qiyang he, jianguo wang, walid g. aref","databases","a recent research trend involves treating database index structures as machine learning (ml) models. in this domain, single or multiple ml models are trained to learn the mapping from keys to positions inside a data set. this class of indexes is known as ""learned indexes."" learned indexes have demonstrated improved search performance and reduced space requirements for one-dimensional data. the concept of one-dimensional learned indexes has naturally been extended to multi-dimensional (e.g., spatial) data, leading to the development of ""learned multi-dimensional indexes"". this survey focuses on learned multi-dimensional index structures. specifically, it reviews the current state of this research area, explains the core concepts behind each proposed method, and classifies these methods based on several well-defined criteria. we present a taxonomy that classifies and categorizes each learned multi-dimensional index, and survey the existing literature on learned multi-dimensional indexes according to this taxonomy. additionally, we present a timeline to illustrate the evolution of research on learned indexes. finally, we highlight several open challenges and future research directions in this emerging and highly active field.",2024-03-11
"43","manifestation of the normal intensity distribution law (nidl) in the rovibrational emission spectrum of hydroxyl radical","emile s. medvedev, aleksander yu. ermilov, vladimir g. ushakov","chemical physics","the latest experimental [noll et al. atmos. chem. phys. 20(2020)5269] and theoretical [brooke et al. j. quant. spectr. rad. transfer 168(2016)142] data on the oh emission intensities are analyzed with use of the nidl. it is found that the calculated intensities of the $\delta v>6$ transitions should not be trusted. the analysis of the oh data revealed that the nidl theory is not applicable to the satellite bands. the effect of small reduced mass previously discovered in h$_2$ [ushakov et al. j. mol. spectrosc. 399(2024)111863], causing the nidl straight-line slope to be larger than the one associated with the repulsive branch of the potential, is demonstrated, and the same should be true of all the diatomic hydrides. we performed ab initio calculations of the oh repulsive branch and compared it with the one of brooke et al. and the other due to varandas and voronin [chem. phys. 194(1995)91]. we found that the ab initio pef closely follows the varandas-voronin potential in the repulsive region important for calculating the overtone intensities [medvedev, j. chem. phys. 137(2012)174307]. assumption is made that different potentials can be used to calculate transition frequencies and intensities for spectroscopic databases.",2024-03-10
"44","texture image retrieval using a classification and contourlet-based features","asal rouhafzay, nadia baaziz, mohand said allili","computer vision and pattern recognition","in this paper, we propose a new framework for improving content based image retrieval (cbir) for texture images. this is achieved by using a new image representation based on the rct-plus transform which is a novel variant of the redundant contourlet transform that extracts a richer directional information in the image. moreover, the process of image search is improved through a learning-based approach where the images of the database are classified using an adapted similarity metric to the statistical modeling of the rct-plus transform. a query is then first classified to select the best texture class after which the retained class images are ranked to select top ones. by this, we have achieved significant improvements in the retrieval rates compared to previous cbir schemes.",2024-03-10
"45","learned 3d volumetric recovery of clouds and its uncertainty for climate analysis","roi ronen, ilan koren, aviad levis, eshkol eytan, vadim holodovsky, yoav y. schechner","computer vision and pattern recognition","significant uncertainty in climate prediction and cloud physics is tied to observational gaps relating to shallow scattered clouds. addressing these challenges requires remote sensing of their three-dimensional (3d) heterogeneous volumetric scattering content. this calls for passive scattering computed tomography (ct). we design a learning-based model (probct) to achieve ct of such clouds, based on noisy multi-view spaceborne images. probct infers - for the first time - the posterior probability distribution of the heterogeneous extinction coefficient, per 3d location. this yields arbitrary valuable statistics, e.g., the 3d field of the most probable extinction and its uncertainty. probct uses a neural-field representation, making essentially real-time inference. probct undergoes supervised training by a new labeled multi-class database of physics-based volumetric fields of clouds and their corresponding images. to improve out-of-distribution inference, we incorporate self-supervised learning through differential rendering. we demonstrate the approach in simulations and on real-world data, and indicate the relevance of 3d recovery and uncertainty to precipitation and renewable energy.",2024-03-09
"46","optimizing llm queries in relational workloads","shu liu, asim biswal, audrey cheng, xiangxi mo, shiyi cao, joseph e. gonzalez, ion stoica, matei zaharia","machine learning","analytical database providers (e.g., redshift, databricks, bigquery) have rapidly added support for invoking large language models (llms) through native user-defined functions (udfs) to help users perform natural language tasks, such as classification, entity extraction, and translation, inside analytical workloads. for instance, an analyst might want to extract customer sentiments on millions of product reviews. however, llm inference is highly expensive in both computational and economic terms: for example, an nvidia l4 gpu running llama2-7b can only process 6 kb of text per second. in this paper, we explore how to optimize llm inference for analytical workloads that invoke llms within relational queries. we show that relational queries present novel opportunities for accelerating llm inference, including reordering rows to maximize key-value (kv) cache reuse within the llm inference engine, reordering columns within a row to further increase cache reuse, and deduplicating redundant inference requests. we implement these optimizations in apache spark, with vllm as the model serving backend and achieve up to 4.4x improvement in end-to-end latency on a benchmark of diverse llm-based queries on real datasets. to the best of our knowledge, this is the first work to explicitly address the problem of optimizing llm invocations within sql queries.",2024-03-09
"47","from electrons to phase diagrams with classical and machine learning potentials: automated workflows for materials science with pyiron","sarath menon, yury lysogorskiy, alexander l. m. knoll, niklas leimeroth, marvin poul, minaam qamar, jan janssen, matous mrovec, jochen rohrer, karsten albe, jörg behler, ralf drautz, jörg neugebauer","materials science","we present a comprehensive and user-friendly framework built upon the pyiron integrated development environment (ide), enabling researchers to perform the entire machine learning potential (mlp) development cycle consisting of (i) creating systematic dft databases, (ii) fitting the density functional theory (dft) data to empirical potentials or mlps, and (iii) validating the potentials in a largely automatic approach. the power and performance of this framework are demonstrated for three conceptually very different classes of interatomic potentials: an empirical potential (embedded atom method - eam), neural networks (high-dimensional neural network potentials - hdnnp) and expansions in basis sets (atomic cluster expansion - ace). as an advanced example for validation and application, we show the computation of a binary composition-temperature phase diagram for al-li, a technologically important lightweight alloy system with applications in the aerospace industry.",2024-03-08
"48","the hertzsprung progression of classical cepheids in the gaia era","marcella marconi, giulia de somma, roberto molinaro, anupam bhardwaj, vincenzo ripepi, ilaria musella, teresa sicignano, erasmo trentin, silvio leccia","solar and stellar astrophysics","a new fine grid of nonlinear convective pulsation models for the so-called ""bump cepheids"" is presented to investigate the hertzprung progression (hp) phenomenon shown by their light and radial pulsation velocity curves. the period corresponding to the center of the hp is investigated as a function of various model assumptions, such as the efficiency of super-adiabatic convection, the mass-luminosity relation, and the metal and helium abundances. the assumed mass-luminosity relation is found to significantly affect the phenomenon but variations in the chemical composition as well as in the stellar mass (at fixed mass-luminosity relation) also play a key role in determining the value of the hp center period. finally, the predictive capability of the presented theoretical scenario is tested against observed light curves of bump cepheids in the esa gaia database, also considering the variation of the pulsation amplitudes and of the fourier parameters $r_{21}$ and $\phi_{21}$ with the pulsation period. a qualitative agreement between theory and observations is found for what concerns the evolution of the light curve morphology as the period moves across the hp center, as well for the pattern in period-amplitude, period-$r21$ and period-$\phi_{21}$ planes. a larger sample of observed cepheids with accurate light curves and metallicities is required in order to derive more quantitative conclusions.",2024-03-08
"49","simulating charged defects at database scale","jimmy-xuan shen, lars f. voss, joel basile varley","materials science","point defects have a strong influence on the physical properties of materials, often dominating the electronic and optical behavior in semiconductors and insulators. the simulation and analysis of point defects is therefore crucial for understanding the growth and operation of materials especially for optoelectronics applications. in this work, we present a general-purpose python framework for the analysis of point defects in crystalline materials, as well as a generalized workflow for their treatment with high-throughput simulations. the distinguishing feature of our approach is an emphasis on a unique, unitcell, structure-only, definition of point defects which decouples the defect definition and the specific supercell representation used to simulate the defect. this allows the results of first-principles calculations to be aggregated into a database without extensive provenance information and is a crucial step in building a persistent database of point defects that can grow over time, a key component towards realizing the idea of a ``defect genome' that can yield more complex relationships governing the behavior of defects in materials. we demonstrate several examples of the approach for three technologically relevant materials and highlight current pitfalls that must be considered when employing these methodologies, as well as their potential solutions.",2024-03-08
"50","piperag: fast retrieval-augmented generation via algorithm-system co-design","wenqi jiang, shuai zhang, boran han, jie wang, bernie wang, tim kraska","computation and language","retrieval-augmented generation (rag) can enhance the generation quality of large language models (llms) by incorporating external token databases. however, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. in this paper, we introduce piperag, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. piperag integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. our evaluation shows that, by combining the three aforementioned methods, piperag achieves up to 2.6$\times$ speedup in end-to-end generation latency while improving generation quality. these promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of piperag in future rag systems.",2024-03-08
"51","fast methods for computing photometric variability of eccentric binaries: boosting, lensing, and variable accretion","daniel j. d'orazio, paul c. duffell, christopher tiede","high energy astrophysical phenomena","we analyze accretion-rate time series for equal-mass binaries in co-planar gaseous disks spanning a continuous range of orbital eccentricities up to 0.8, for both prograde and retrograde systems. the dominant variability timescales match that of previous investigations; the binary orbital period is dominant for prograde binaries with $e \gtrsim 0.1$, with a 5 times longer ""lump"" period taking over for $e\lesssim 0.1$. this lump period fades and drops from 5 times to 4.5 times the binary period as $e$ approaches 0.1, where it vanishes. for retrograde orbits, the binary orbital period dominates at $e \lesssim 0.55$ and is accompanied by a 2 times longer-timescale periodicity at higher eccentricities. the shape of the accretion-rate time series varies with binary eccentricity. for prograde systems, the orientation of an eccentric disk causes periodic trading of accretion between the binary components in a ratio that we report as a function of binary eccentricity. we present a publicly available tool, binlite, that can rapidly ($\lesssim 0.01$~sec) generate templates for the accretion-rate time series, onto either binary component, for choice of binary eccentricity below 0.8. as an example use-case, we build lightcurve models where the accretion rate through the circumbinary disk and onto each binary component sets contributions to the emitted specific flux. we combine these rest-frame, accretion-variability lightcurves with observer-dependent doppler boosting and binary self-lensing. this allows a flexible approach to generating lightcurves over a wide range of binary and observer parameter space. we envision binlite as the access point to a living database that will be updated with state-of-the-art hydrodynamical calculations as they advance.",2024-03-08
"52","eternal sunshine of the mechanical mind: the irreconcilability of machine learning and the right to be forgotten","meem arafat manab","general literature","as we keep rapidly advancing toward an era where artificial intelligence is a constant and normative experience for most of us, we must also be aware of what this vision and this progress entail. by first approximating neural connections and activities in computer circuits and then creating more and more sophisticated versions of this crude approximation, we are now facing an age to come where modern deep learning-based artificial intelligence systems can rightly be called thinking machines, and they are sometimes even lauded for their emergent behavior and black-box approaches. but as we create more powerful electronic brains, with billions of neural connections and parameters, can we guarantee that these mammoths built of artificial neurons will be able to forget the data that we store in them? if they are at some level like a brain, can the right to be forgotten still be protected while dealing with these ais? the essential gap between machine learning and the rtbf is explored in this article, with a premonition of far-reaching conclusions if the gap is not bridged or reconciled any time soon. the core argument is that deep learning models, due to their structure and size, cannot be expected to forget or delete a data as it would be expected from a tabular database, and they should be treated more like a mechanical brain, albeit still in development.",2024-03-06
"53","nurses as agents for achieving environmentally sustainable health systems: a bibliometric analysis","olga maria luque alcaraz, pilar aparicio-martínez, antonio gomera, manuel vaquero-abellán","computers and society","objective: to analyze the current scientific knowledge and research lines focused on environmentally sustainable health systems, including the role of nurses. background: there seem to be differences between creating interventions focused on environmentally sustainable health systems, including nurses, and the scarcity of research on this topic, framed on the sustainable development goals. methods: a bibliometric analysis was carried out, via three databases (web of science, scopus, and pubmed), and the guideline recommendations were followed to select bibliometric data. results: the search resulted in 159 publications, significantly increasing the trends from 2017 to 2021 (p=0.028). the most relevant countries in this area were the united states of america, the united kingdom, and sweden. also, the top articles were from relevant journals, indexed in journal citation report, and the first and the second quartile linked to the nursing field and citations (p<0.001). conclusion: education is key to achieving environmentally sustainable health systems via institutions and policies. implications for nursing management: there is a lack of experimental data and policies on achieving or maintaining environmentally sustainable health care systems, indicating that nurses have an important role and should be consulted and included in decision-making policies regarding sustainability in the healthcare systems.",2024-02-05
"54","erbench: an entity-relationship based automatically verifiable hallucination benchmark for large language models","jio oh, soyeon kim, junseok seo, jindong wang, ruochen xu, xing xie, steven euijong whang","computation and language","large language models (llms) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. we contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. we propose erbench to automatically convert any relational database into a benchmark based on the entity-relationship (er) model. our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. in addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of llms. finally, erbench supports continuous evaluation, multimodal questions, and various prompt engineering techniques. in our experiments, we construct an llm benchmark using databases of multiple domains and make an extensive comparison of contemporary llms. we observe that better llms like gpt-4 can handle a larger variety of question types, but are by no means perfect. also, correct answers do not necessarily imply correct rationales, which is an important evaluation that erbench does better than other benchmarks for various question types. code is available at https: //github.com/dilab-kaist/erbench.",2024-03-08
"55","fairness-aware interpretable modeling (faim) for trustworthy machine learning in healthcare","mingxuan liu, yilin ning, yuhe ke, yuqing shang, bibhas chakraborty, marcus eng hock ong, roger vaughan, nan liu","machine learning","the escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. we propose an interpretable framework - fairness-aware interpretable modeling (faim), to improve model fairness without compromising performance, featuring an interactive interface to identify a ""fairer"" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. we demonstrated faim's value in reducing sex and race biases by predicting hospital admission with two real-world databases, mimic-iv-ed and sgh-ed. we show that for both datasets, faim models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established fairness metrics, outperforming commonly used bias-mitigation methods. our approach demonstrates the feasibility of improving fairness without sacrificing performance and provides an a modeling mode that invites domain experts to engage, fostering a multidisciplinary effort toward tailored ai fairness.",2024-03-08
"56","interoperability of the metaverse: a digital ecosystem perspective review","liang yang, shi-ting ni, yuyang wang, ao yu, jyh-an lee, pan hui","computers and society","the metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles. however, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress. interoperability, recognized as a major barrier to the metaverse's full potential, is central to this debate. coinmarketcap's report in february 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge. despite consensus on its critical role, there is a research gap in exploring the impact on the metaverse, significance, and developmental extent. our study bridges this gap via a systematic literature review and content analysis of the web of science (wos) and scopus databases, yielding 74 publications after a rigorous selection process. interoperability, difficult to define due to varied contexts and lack of standardization, is central to the metaverse, often seen as a digital ecosystem. urs gasser's framework from harvard law school, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities. incorporating this framework, we dissect literature for a comprehensive metaverse interoperability overview. our study seeks to establish benchmarks for future inquiries, navigating the complex field of metaverse interoperability studies and contributing to academic advancement.",2024-03-08
"57","correlation analysis technique of key parameters for transformer material inspection based on fp-tree and knowledge graph","jing xu, yongbo zhang","systems and control","as one of the key equipment in the distribution system, the distribution transformer directly affects the reliability of the user power supply. the probability of accidents occurring in the operation of transformer equipment is high, so it has become a focus of material inspection in recent years. however, the large amount of raw data from sample testing is not being used effectively. given the above problems, this paper aims to mine the relationship between the unqualified distribution transformer inspection items by using the association rule algorithm based on the distribution transformer inspection data collected from 2017 to 2021 and sorting out the key inspection items. at the same time, the unqualified judgment basis of the relevant items is given, and the internal relationship between the inspection items is clarified to a certain extent. furthermore, based on material and equipment inspection reports, correlations between failed inspection items, and expert knowledge, the knowledge graph of material equipment inspection management is constructed in the graph database neo4j. the experimental results show that the fp-growth method performs significantly better than the apriori method and can accurately assess the relationship between failed distribution transformer inspection items. finally, the knowledge graph network is visualized to provide a systematic knowledge base for material inspection, which is convenient for knowledge query and management. this method can provide a scientific guidance program for operation and maintenance personnel to do equipment maintenance and also offers a reference for the state evaluation of other high-voltage equipment.",2024-03-08
"58","np-completeness for the space-optimality of double-array tries","hideo bannai, keisuke goto, shunsuke kanda, dominik köppl","data structures and algorithms","indexing a set of strings for prefix search or membership queries is a fundamental task with many applications such as information retrieval or database systems. a classic abstract data type for modelling such an index is a trie. due to the fundamental nature of this problem, it has sparked much interest, leading to a variety of trie implementations with different characteristics. a trie implementation that has been well-used in practice is the double-array (trie) consisting of merely two integer arrays. while a traversal takes constant time per node visit, the needed space consumption in computer words can be as large as the product of the number of nodes and the alphabet size. despite that several heuristics have been proposed on lowering the space requirements, we are unaware of any theoretical guarantees. in this paper, we study the decision problem whether there exists a double-array of a given size. to this end, we first draw a connection to the sparse matrix compression problem, which makes our problem np-complete for alphabet sizes linear to the number of nodes. we further propose a reduction from the restricted directed hamiltonian path problem, leading to np-completeness even for logarithmic-sized alphabets.",2024-03-07
"59","evaluation of nosql in the energy marketplace with graphql optimization","michael howard","databases","the growing popularity of electric vehicles in the united states requires an ever-expanding infrastructure of commercial dc fast charging stations. the u.s. department of energy estimates 33,355 publicly available dc fast charging stations as of september 2023. range anxiety is an important impediment to the adoption of electric vehicles and is even more relevant in underserved regions in the country. the peer-to-peer energy marketplace helps fill the demand by allowing private home and small business owners to rent their 240 volt, level-2 charging facilities. the existing, publicly accessible outlets are wrapped with a cloud-connected microcontroller managing security and charging sessions. these microcontrollers act as edge devices communicating with a cloud message broker, while both buyer and seller users interact with the framework via a web-based user interface. the database storage used by the marketplace framework is a key component in both the cost of development and the performance that contributes to the user experience. a traditional storage solution is the sql database. however, difficulty in scaling across multiple nodes and cost of its server-based compute have resulted in a trend in the last 20 years towards other nosql, serverless approaches. in this study, we evaluate the nosql vs. sql solutions through a comparison of google cloud firestore and cloud sql mysql offerings. the comparison pits google's serverless, document-model, non-relational, nosql against the server-base, table-model, relational, sql service. the evaluation is based on query latency, flexibility/scalability, and cost criteria. through benchmarking and analysis of the architecture, we determine whether firestore can support the energy marketplace storage needs and if the introduction of a graphql middleware layer can overcome its deficiencies.",2024-03-07
"60","large language multimodal models for 5-year chronic disease cohort prediction using ehr data","jun-en ding, phan nguyen minh thao, wen-chih peng, jian-zhe wang, chun-cheng chug, min-chen hsieh, yun-chien tseng, ling chen, dongsheng luo, chi-te wang, pei-fu chen, feng liu, fang-ming hung","computation and language","chronic diseases such as diabetes are the leading causes of morbidity and mortality worldwide. numerous research studies have been attempted with various deep learning models in diagnosis. however, most previous studies had certain limitations, including using publicly available datasets (e.g. mimic), and imbalanced data. in this study, we collected five-year electronic health records (ehrs) from the taiwan hospital database, including 1,420,596 clinical notes, 387,392 laboratory test results, and more than 1,505 laboratory test items, focusing on research pre-training large language models. we proposed a novel large language multimodal models (llmms) framework incorporating multimodal data from clinical notes and laboratory test results for the prediction of chronic disease risk. our method combined a text embedding encoder and multi-head attention layer to learn laboratory test values, utilizing a deep neural network (dnn) module to merge blood features with chronic disease semantics into a latent space. in our experiments, we observe that clinicalbert and pubmed-bert, when combined with attention fusion, can achieve an accuracy of 73% in multiclass chronic diseases and diabetes prediction. by transforming laboratory test values into textual descriptions and employing the flan t-5 model, we achieved a 76% area under the roc curve (auroc), demonstrating the effectiveness of leveraging numerical text data for training and inference in language models. this approach significantly improves the accuracy of early-stage diabetes prediction.",2024-03-02
"61","what cannot be skipped about the skiplist: a survey of skiplists and their applications in big data systems","venkata sai pavan kumar vadrevu, lu xing, walid g. aref","databases","skiplists have become prevalent in systems. the main advantages of skiplists are their simplicity and ease of implementation, and the ability to support operations in the same asymptotic complexities as their tree-based counterparts. in this survey, we explore skiplists and their many variants. we highlight many scenarios of how skiplists are useful and fit well in these usage scenarios. we study several extensions to skiplists to make them fit for more applications, e.g., their use in the multi-dimensional space, network overlaying algorithms, as well as serving as indexes in database systems. besides, we also discuss systems that adopt the idea of skiplists and apply the probabilistic skip pattern into their designs.",2024-03-07
"62","children age group detection based on human-computer interaction and time series analysis","juan carlos ruiz-garcia, carlos hojas, ruben tolosana, ruben vera-rodriguez, aythami morales, julian fierrez, javier ortega-garcia, jaime herreros-rodriguez","human-computer interaction","this article proposes a novel children-computer interaction (cci) approach for the task of age group detection. this approach focuses on the automatic analysis of the time series generated from the interaction of the children with mobile devices. in particular, we extract a set of 25 time series related to spatial, pressure, and kinematic information of the children interaction while colouring a tree through a pen stylus tablet, a specific test from the large-scale public childcidb database. a complete analysis of the proposed approach is carried out using different time series selection techniques to choose the most discriminative ones for the age group detection task: i) a statistical analysis, and ii) an automatic algorithm called sequential forward search (sfs). in addition, different classification algorithms such as dynamic time warping barycenter averaging (dba) and hidden markov models (hmm) are studied. accuracy results over 85% are achieved, outperforming previous approaches in the literature and in more challenging age group conditions. finally, the approach presented in this study can benefit many children-related applications, for example, towards an age-appropriate environment with the technology.",2024-03-07
"63","dissociative recombination of ns+ in collisions with slow electrons","r. hassaine, f. gauchet, f. iacob, j. zs mezei, e. roueff, j. tennyson, i. f. schneider","instrumentation and methods for astrophysics","cross sections and rate coefficients for the dissociative recombination (dr) of the ns+ ion induced by collisions with low-energy electrons are reported for temperatures between 10 and 1000 k, relevant to a large range of interstellar cloud temperatures. uncertainties are discussed for these rates. comparisons are made with dr rates for the isovalent no+ molecular ion which are found to be much faster. the present findings lead to a moderate dissociative reaction rate coefficient, smaller by a factor of 2 than the current estimates reported in the different kinetic databases for a temperature of 10 k. we consider that our rate coefficients obtained through multichannel quantum defect theory for ns+ are likely to be better than those displayed in the different kinetic databases.",2024-03-07
"64","on $(r,c)$-constant, planar and circulant graphs","yair caro, xandru mifsud","combinatorics","this paper concerns $(r,c)$-constant graphs, which are $r$-regular graphs in which the subgraph induced by the open neighbourhood of every vertex has precisely $c$ edges. the family of $(r,c)$-graphs contains vertex-transitive graphs (and in particular cayley graphs), graphs with constant link (sometimes called locally isomorphic graphs), $(r,b)$-regular graphs, strongly regular graphs, and much more. this family was recently introduced in [arxiv:2312.08777] serving as important tool in constructing flip graphs [arxiv:2312.08777, arxiv:2401.02315]. in this paper we shall mainly deal with the following: i. existence and non-existence of $(r, c)$-planar graphs. we completely determine the cases of existence and non-existence of such graphs and supply the smallest order in the case when they exist. ii. we consider the existence of $(r, c)$-circulant graphs. we prove that for $c \equiv 2 \ (\mathrm{mod} \ 3)$ no $(r,c)$-circulant graph exists and that for $c \equiv 0, 1 \ (\mathrm{mod} \ 3)$, $c > 0$ and $r \geq 6 + \sqrt{\frac{8c - 5}{3}}$ there exists $(r,c)$-circulant graphs. moreover for $c = 0$ and $r \geq 1$, $(r, 0)$-circulants exist. iii. we consider the existence and non-existence of small $(r,c)$-constant graphs, supplying a complete table of the smallest order of graphs we found for $0 \leq c \leq \binom{r}{2}$ and $r \leq 6$. we shall also determine all the cases in this range for which $(r,c)$-constant graphs don't exist. we establish a public database of $(r,c)$-constant graphs for varying $r$, $c$ and order.",2024-03-07
"65","exploring llm-based agents for root cause analysis","devjeet roy, xuchao zhang, rashi bhave, chetan bansal, pedro las-casas, rodrigo fonseca, saravan rajmohan","software engineering","the growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. root cause analysis (rca), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. automation of rca can result in significant savings of time, and ease the burden of incident management on on-call engineers. recently, researchers have utilized large language models (llms) to perform rca, and have demonstrated promising results. however, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. in this work, we explore the use of llm based agents for rca to address this limitation. we present a thorough empirical evaluation of a react agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at microsoft. results show that react performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. we then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. lastly, we conduct a case study with a team at microsoft to equip the react agent with tools that give it access to external diagnostic services that are used by the team for manual rca. our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.",2024-03-07
"66","promptcharm: text-to-image generation through multi-modal prompting and refinement","zhijie wang, yuheng huang, da song, lei ma, tianyi zhang","human-computer interaction","the recent advancements in generative ai have significantly advanced the field of text-to-image generation. the state-of-the-art text-to-image model, stable diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. crafting text prompts that align with the model's interpretation and the user's intent thus becomes crucial. however, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. to address these challenges, we propose promptcharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. to assist novice users in prompting, promptcharm first automatically refines and optimizes the user's initial prompt. furthermore, promptcharm supports the user in exploring and selecting different image styles within a large database. to assist users in effectively refining their prompts and images, promptcharm renders model explanations by visualizing the model's attention values. if the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of promptcharm. to evaluate the effectiveness and usability of promptcharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. these two studies show that participants using promptcharm were able to create images with higher quality and better aligned with the user's expectations compared with using two variants of promptcharm that lacked interaction or visualization support.",2024-03-06
"67","a survey on adversarial contention resolution","ioana banicescu, trisha chakraborty, seth gilbert, maxwell young","distributed, parallel, and cluster computing","contention resolution addresses the challenge of coordinating access by multiple processes to a shared resource such as memory, disk storage, or a communication channel. originally spurred by challenges in database systems and bus networks, contention resolution has endured as an important abstraction for resource sharing, despite decades of technological change. here, we survey the literature on resolving worst-case contention, where the number of processes and the time at which each process may start seeking access to the resource is dictated by an adversary. we highlight the evolution of contention resolution, where new concerns -- such as security, quality of service, and energy efficiency -- are motivated by modern systems. these efforts have yielded insights into the limits of randomized and deterministic approaches, as well as the impact of different model assumptions such as global clock synchronization, knowledge of the number of processors, feedback from access attempts, and attacks on the availability of the shared resource.",2024-03-06
"68","accurate reference spectra of hd in h$_2$/he bath for planetary applications","h. jóźwiak, n. stolarczyk, k. stankiewicz, m. zaborowski, d. lisak, s. wójtewicz, p. jankowski, k. patkowski, k. szalewicz, f. thibault, i.e. gordon, p. wcisło","earth and planetary astrophysics","the hydrogen deuteride (hd) molecule is an important deuterium tracer in astrophysical studies. the atmospheres of gas giants are dominated by molecular hydrogen, and simultaneous observation of h$_2$ and hd lines provides reliable information on the d/h ratios on these planets. the reference spectroscopic parameters play a crucial role in such studies. under thermodynamic conditions encountered in these atmospheres, the spectroscopic studies of hd require not only the knowledge of line intensities and positions but also accurate reference data on pressure-induced line shapes and shifts. our aim is to provide accurate collision-induced line-shape parameters for hd lines that cover any thermodynamic conditions relevant to the atmospheres of giant planets, i.e., any relevant temperature, pressure, and perturbing gas (the h$_2$/he mixture) composition. we perform quantum-scattering calculations on a new highly accurate ab initio potential energy surface, and we use scattering s-matrices obtained this way to determine the collision-induced line-shape parameters. we use the cavity ring-down spectroscopy for validation of our theoretical methodology. we report accurate collision-induced line-shape parameters for the pure rotational r(0), r(1), and r(2) lines, the most relevant hd lines for the investigations of atmospheres of the giant planets. besides the basic voigt-profile collisional parameters (i.e. the broadening and shift parameters), we also report their speed dependences and the complex dicke parameter, which can influence the effective width and height of the hd lines up to almost a factor of 2 for giant planet conditions. the sub-percent-level accuracy, reached in this work, considerably improves the previously available data. all the reported parameters are consistent with the hitran database format, hence allowing for the use of hapi for generating the beyond-voigt spectra of hd.",2024-03-06
"69","operational space and plasma performance with an rmp-elm suppressed edge","c. paz-soldan, s. gu, n. leuthold, p. lunia, p. xie, m.w. kim, s.k. kim, n.c. logan, j.-k. park, w. suttrop, y. sun, d.b. weisberg, m. willensdorfer, the asdex-upgrade, diii-d, east, kstar teams","plasma physics","the operational space and global performance of plasmas with edge-localized modes (elms) suppressed by resonant magnetic perturbations (rmps) are surveyed by comparing aug, diii-d, east, and kstar stationary operating points. rmp-elm suppression is achieved over a range of plasma currents, toroidal fields, and rmp toroidal mode numbers. consistent operational windows in edge safety factor are found across devices, while windows in plasma shaping parameters are distinct. accessed pedestal parameters reveal a quantitatively similar pedestal-top density limit for rmp-elm suppression in all devices of just over 3x1019 m-3. this is surprising given the wide variance of many engineering parameters and edge collisionalities, and poses a challenge to extrapolation of the regime. wide ranges in input power, confinement time, and stored energy are observed, with the achieved triple product found to scale like the product of current, field, and radius. observed energy confinement scaling with engineering parameters for rmp-elm suppressed plasmas are presented and compared with expectations from established h and l-mode scalings, including treatment of uncertainty analysis. different scaling exponents for individual engineering parameters are found as compared to the established scalings. however, extrapolation to next-step tokamaks iter and sparc find overall consistency within uncertainties with the established scalings, finding no obvious performance penalty when extrapolating from the assembled multi-device rmp-elm suppressed database. overall this work identifies common physics for rmp-elm suppression and highlights the need to pursue this no-elm regime at higher magnetic field and different plasma physical size.",2024-03-06
"70","a field- and time-normalized bayesian approach to measuring the impact of a publication","emilio gómez-déniz, pablo dorta-gonzález","digital libraries","measuring the impact of a publication in a fair way is a significant challenge in bibliometrics, as it must not introduce biases between fields and should enable comparison of the impact of publications from different years. in this paper, we propose a bayesian approach to tackle this problem, motivated by empirical data demonstrating heterogeneity in citation distributions. the approach uses the a priori distribution of citations in each field to estimate the expected a posteriori distribution in that field. this distribution is then employed to normalize the citations received by a publication in that field. our main contribution is the bayesian impact score, a measure of the impact of a publication. this score is increasing and concave with the number of citations received and decreasing and convex with the age of the publication. this means that the marginal score of an additional citation decreases as the cumulative number of citations increases and increases as the time since publication of the document grows. finally, we present an empirical application of our approach in eight subject categories using the scopus database and a comparison with the normalized impact indicator field citation ratio from the dimensions ai database.",2024-03-06
"71","comparison performance of spectrogram and scalogram as input of acoustic recognition task","dang thoai phan, andre jakob, marcus purat","audio and speech processing","acoustic recognition is a common task for deep learning in recent researches, with the employment of spectral feature extraction such as short-time fourier transform and wavelet transform. however, not many researches have found that discuss the advantages and drawbacks, as well as performance comparison amongst spectral feature extractors. in this consideration, this paper aims to comparing the attributes of these two transform types, called spectrogram and scalogram. a convolutional neural networks for acoustic faults recognition is implemented, then the performance of these two types of spectral extractor is recorded for comparison. a latest research on the same audio database is considered for benchmarking to see how good the designed spectrogram and scalogram is. the advantages and limitations of them are also analyzed. by doing so, the results of this paper provide indications for application scenarios of spectrogram and scalogram, as well as potential further research directions in acoustic recognition.",2024-03-06
"72","modelling the inelastic constitutive behaviour of multi-layer spiral strands. comparison of hysteresis operator approach to multi-scale model","davide manfredo, mohammad ali saadat, vanessa dörlich, joachim linn, damien durville, martin arnold","materials science","the simulation of inelastic effects in flexible slender technical devices has become of increasing interest in the past years. different approaches have been considered depending on the effects relevant for the specific application. recently, a mixed stress strain driven computational homogenisation has been proposed to model the dissipative nonlinear bending response of spiral strands subjected to axial force. in this study, we propose two different approaches, namely a rheological model and a databased greybox model, to predict the cyclic response of these strands using only their monotonic response. in the first approach, a system of so-called bending springs and sliders is used to model different contributions to the bending stiffness of the strands. the data-based approach makes use of mathematical tools called hysteresis operators. the prandtl-ishlinskii operator plays a relevant role in modelling the input-output relation in phenomena showing hysteretic behaviour and can be expressed as a weighted superposition of elementary stop operators. comparing the two approaches leads to a better understanding and an explicit physical interpretation of the parameters of a specific class of hysteresis operator models.",2024-03-06
"73","metamat 01: a semi-analytic solution for benchmarking wave propagation simulations of homogeneous absorbers in 1d/3d and 2d","stefan schoder, paul maurerlehner","sound","the development of acoustic simulation workflows in the time-domain description is essential for predicting the sound of aeroacoustic or other transient acoustic effects. a common practice for noise mitigation is using absorbers. the modeling of these acoustic absorbers is typically provided in the frequency domain. several, methods established bridging this gap, investigating methods to model absorber in the time domain. therefore, this short article, describes the analytic solution in time-domain for benchmarking absorber simulations with infinite 1d, 2d, and 3d domains. connected to the analytic solution, a matlab script is provided to easily obtain the reference solution. the reference codes are provided as benchmark solution in the eaa tcca benchmarking database as metamat 01.",2024-03-06
"74","f$^3$loc: fusion and filtering for floorplan localization","changan chen, rui wang, christoph vogel, marc pollefeys","computer vision and pattern recognition","in this paper we propose an efficient data-driven solution to self-localization within a floorplan. floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. our method does not require retraining per map and location or demand a large database of images of the area of interest. we propose a novel probabilistic model consisting of an observation and a novel temporal filtering module. operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.",2024-03-05
"75","fine-grained privacy guarantees for coverage problems","laxman dhulipala, george z. li","data structures and algorithms","we introduce a new notion of neighboring databases for coverage problems such as max cover and set cover under differential privacy. in contrast to the standard privacy notion for these problems, which is analogous to node-privacy in graphs, our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy. we illustrate several scenarios of set cover and max cover where our privacy notion is desired one for the application. our main result is an $\epsilon$-edge differentially private algorithm for max cover which obtains an $(1-1/e-\eta,\tilde{o}(k/\epsilon))$-approximation with high probability. furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\omega(k/\epsilon)$ is necessary under edge-differential privacy. via group privacy properties, this implies a new algorithm for $\epsilon$-node differentially private max cover which obtains an $(1-1/e-\eta,\tilde{o}(fk/\epsilon))$-approximation, where $f$ is the maximum degree of an element in the set system. when $f\ll k$, this improves over the best known algorithm for max cover under pure (node) differential privacy, which obtains an $(1-1/e,\tilde{o}(k^2/\epsilon))$-approximation.",2024-03-05
"76","ai insights: a case study on utilizing chatgpt intelligence for research paper analysis","anjalee de silva, janaka l. wijekoon, rashini liyanarachchi, rrubaa panchendrarajan, weranga rajapaksha","artificial intelligence","this paper discusses the effectiveness of leveraging chatbot: generative pre-trained transformer (chatgpt) versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys. the study selected the \textit{application of artificial intelligence in breast cancer treatment} as the research topic. research papers related to this topic were collected from three major publication databases google scholar, pubmed, and scopus. chatgpt models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to breast cancer treatment (bct), organization of papers according to scope, and identification of key information for survey paper writing. evaluations performed using ground truth data annotated using subject experts reveal, that gpt-4 achieves 77.3\% accuracy in identifying the research paper categories and 50\% of the papers were correctly identified by gpt-4 for their scopes. further, the results demonstrate that gpt-4 can generate reasons for its decisions with an average of 27\% new words, and 67\% of the reasons given by the model were completely agreeable to the subject experts.",2024-03-05
"77","multiwinner elections and the spoiler effect","david mccune, jennifer wilson","methodology","in the popular debate over the use of ranked-choice voting, it is often claimed that the method of single transferable vote (stv) is immune or mostly immune to the so-called ``spoiler effect,'' where the removal of a losing candidate changes the set of winners. this claim has previously been studied only in the single-winner case. we investigate how susceptible stv is to the spoiler effect in multiwinner elections, where the output of the voting method is a committee of size at least two. to evaluate stv we compare it to numerous other voting methods including single non-transferable vote, $k$-borda, and the chamberlin-courant rule. we provide simulation results under three different random models and empirical results using a large database of real-world multiwinner political elections from scotland. our results show that stv is not spoiler-proof in any meaningful sense in the multiwinner context, but it tends to perform well relative to other methods, especially when using real-world ballot data.",2024-03-03
"78","magid: an automated pipeline for generating synthetic multi-modal datasets","hossein aboutalebi, hwanjun song, yusheng xie, arshit gupta, justin sun, hang su, igor shalyminov, nikolaos pappas, siffi singh, saab mansour","computation and language","development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for llms. previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. in this work, we introduce \textbf{m}ultimodal \textbf{a}ugmented \textbf{g}enerative \textbf{i}mages \textbf{d}ialogues (magid), a framework to augment text-only dialogues with diverse and high-quality images. subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. finally, magid incorporates an innovative feedback loop between an image description generation module (textual llm) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. we compare magid to other sota baselines on three dialogue datasets, using automated and human evaluation. our results show that magid is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.",2024-03-05
"79","solving the bongard-logo problem by modeling a probabilistic model","ruizhuo song, beiming yuan","computer vision and pattern recognition","abstract reasoning problems challenge the perceptual and cognitive abilities of ai algorithms, demanding deeper pattern discernment and inductive reasoning beyond explicit image features. this study introduces pmoc, a tailored probability model for the bongard-logo problem, achieving high reasoning accuracy by constructing independent probability models. additionally, we present pose-transformer, an enhanced transformer-encoder designed for complex abstract reasoning tasks, including bongard-logo, raven, i-raven, and pgm. pose-transformer incorporates positional information learning, inspired by capsule networks' pose matrices, enhancing its focus on local positional relationships in image data processing. when integrated with pmoc, it further improves reasoning accuracy. our approach effectively addresses reasoning difficulties associated with abstract entities' positional changes, outperforming previous models on the oig, d3$\times$3 subsets of raven, and pgm databases. this research contributes to advancing ai's capabilities in abstract reasoning and cognitive pattern recognition.",2024-03-05
"80","a3cosmos & a3goodss: continuum source catalogues and multi-band number counts","sylvia adscheid, benjamin magnelli, daizhong liu, frank bertoldi, ivan delvecchio, carlotta gruppioni, eva schinnerer, alberto traina, matthieu béthermin, athanasia gkogkou","astrophysics of galaxies","galaxy submillimetre number counts are a fundamental measurement in our understanding of galaxy evolution models. most early measurements are obtained via single-dish telescopes with substantial source confusion, whereas recent interferometric observations are limited to small areas. we used a large database of alma continuum observations to accurately measure galaxy number counts in multiple (sub)millimetre bands, thus bridging the flux density range between single-dish surveys and deep interferometric studies. we continued the automated mining of the alma archive in the cosmos field project (a3cosmos) and extended it with observations from the goods-south field (a3goodss). the database consists of ~4,000 pipeline-processed continuum images from the public alma archive, yielding 2,050 unique detected sources. to infer galaxy number counts, we constructed a method to reduce the observational bias inherent to targeted pointings that dominate the database. this method comprises a combination of image selection, masking, and source weighting. the effective area was calculated by accounting for inhomogeneous wavelengths, sensitivities, and resolutions and for spatial overlap between images. we tested and calibrated our method with simulations. we obtained the first number counts derived in a consistent and homogeneous way in four different alma bands covering a relatively large area. the results are consistent with number counts from the literature within the uncertainties. we extended the available depth in alma band 4 by 0.4 dex with respect to previous studies. in band 7, at the depth of the inferred number counts, ~40% of the cosmic infrared background is resolved into discrete sources. this fraction, however, decreases with wavelength, reaching ~4% in band 3. finally, we used the number counts to test models of dusty galaxy evolution, and find a good agreement within the uncertainties.",2024-03-05
"81","scalable bayesian inference for the generalized linear mixed model","samuel i. berchuck, felipe a. medeiros, sayan mukherjee, andrea agazzi","computation","the generalized linear mixed model (glmm) is a popular statistical approach for handling correlated data, and is used extensively in applications areas where big data is common, including biomedical data settings. the focus of this paper is scalable statistical inference for the glmm, where we define statistical inference as: (i) estimation of population parameters, and (ii) evaluation of scientific hypotheses in the presence of uncertainty. artificial intelligence (ai) learning algorithms excel at scalable statistical estimation, but rarely include uncertainty quantification. in contrast, bayesian inference provides full statistical inference, since uncertainty quantification results automatically from the posterior distribution. unfortunately, bayesian inference algorithms, including markov chain monte carlo (mcmc), become computationally intractable in big data settings. in this paper, we introduce a statistical inference algorithm at the intersection of ai and bayesian inference, that leverages the scalability of modern ai algorithms with guaranteed uncertainty quantification that accompanies bayesian inference. our algorithm is an extension of stochastic gradient mcmc with novel contributions that address the treatment of correlated data (i.e., intractable marginal likelihood) and proper posterior variance estimation. through theoretical and empirical results we establish our algorithm's statistical inference properties, and apply the method in a large electronic health records database.",2024-03-05
"82","fuzzy datalog$^\exists$ over arbitrary t-norms","matthias lanzinger, stefano sferrazza, przemysław a. wałęga, georg gottlob","artificial intelligence","one of the main challenges in the area of neuro-symbolic ai is to perform logical reasoning in the presence of both neural and symbolic data. this requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more. to allow for such reasoning, we generalise the standard rule-based language datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies. the resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard datalog setting. in particular, we provide fuzzy extensions of datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, reasoning has the same complexity as in the classical setting.",2024-03-05
"83","quantum data management: from theory to opportunities","rihan hai, shih-han hung, sebastian feld","databases","quantum computing has emerged as a transformative tool for future data management. classical problems in database domains, including query optimization, data integration, and transaction management, have recently been addressed using quantum computing techniques. this tutorial aims to establish the theoretical foundation essential for enhancing methodologies and practical implementations in this line of research. moreover, this tutorial takes a forward-looking approach by delving into recent strides in quantum internet technologies and the nonlocality theory. we aim to shed light on the uncharted territory of future data systems tailored for the quantum internet.",2024-03-05
"84","g4-attention: deep learning model with attention for predicting dna g-quadruplexes","shrimon mukherjee, pulakesh pramanik, partha basuchowdhuri, santanu bhattacharya","machine learning","g-quadruplexes are the four-stranded non-canonical nucleic acid secondary structures, formed by the stacking arrangement of the guanine tetramers. they are involved in a wide range of biological roles because of their exceptionally unique and distinct structural characteristics. after the completion of the human genome sequencing project, a lot of bioinformatic algorithms were introduced to predict the active g4s regions \textit{in vitro} based on the canonical g4 sequence elements, g-\textit{richness}, and g-\textit{skewness}, as well as the non-canonical sequence features. recently, sequencing techniques like g4-seq and g4-chip-seq were developed to map the g4s \textit{in vitro}, and \textit{in vivo} respectively at a few hundred base resolution. subsequently, several machine learning approaches were developed for predicting the g4 regions using the existing databases. however, their prediction models were simplistic, and the prediction accuracy was notably poor. in response, here, we propose a novel convolutional neural network with bi-lstm and attention layers, named g4-attention, to predict the g4 forming sequences with improved accuracy. g4-attention achieves high accuracy and attains state-of-the-art results in the g4 prediction task. our model also predicts the g4 regions accurately in the highly class-imbalanced datasets. in addition, the developed model trained on the human genome dataset can be applied to any non-human genome dna sequences to predict the g4 formation propensities.",2024-03-05
"85","deepbioisostere: discovering bioisosteres with deep learning for a fine control of multiple molecular properties","hyeongwoo kim, seokhyun moon, wonho zhung, jaechang lim, woo youn kim","biomolecules","optimizing molecules to improve their properties is a fundamental challenge in drug design. for a fine-tuning of molecular properties without losing bio-activity validated in advance, the concept of bioisosterism has emerged. many in silico methods have been proposed for discovering bioisosteres, but they require expert knowledge for their applications or are restricted to known databases. here, we introduce deepbioisostere, a deep generative model to design suitable bioisosteric replacements. our model allows an end-to-end chemical replacement by intelligently selecting fragments for removal and insertion along with their attachment orientation. through various scenarios of multiple property control, we showcase the model's capability to modulate specific properties, addressing the challenge in molecular optimization. our model's innovation lies in its capacity to design a bioisosteric replacement reflecting the compatibility with the surroundings of the modification site, facilitating the control of sophisticated properties like drug-likeness. deepbioisostere can also provide previously unseen bioisosteric replacements, highlighting its capability for exploring diverse chemical modifications rather than just mining them from known databases. lastly, we employed deepbioisostere to improve the sensitivity of a known sars-cov-2 main protease inhibitor to the e166v mutant that exhibits drug resistance to the inhibitor, demonstrating its potential application in lead optimization.",2024-03-05
"86","a dual-level cancelable framework for palmprint verification and hack-proof data storage","ziyuan yang, ming kang, andrew beng jin teoh, chengrui gao, wen chen, bob zhang, yi zhang","cryptography and security","in recent years, palmprints have been widely used for individual verification. the rich privacy information in palmprint data necessitates its protection to ensure security and privacy without sacrificing system performance. existing systems often use cancelable technologies to protect templates, but these technologies ignore the potential risk of data leakage. upon breaching the system and gaining access to the stored database, a hacker could easily manipulate the stored templates, compromising the security of the verification system. to address this issue, we propose a dual-level cancelable palmprint verification framework in this paper. specifically, the raw template is initially encrypted using a competition hashing network with a first-level token, facilitating the end-to-end generation of cancelable templates. different from previous works, the protected template undergoes further encryption to differentiate the second-level protected template from the first-level one. the system specifically creates a negative database (ndb) with the second-level token for dual-level protection during the enrollment stage. reversing the ndb is np-hard and a fine-grained algorithm for ndb generation is introduced to manage the noise and specified bits. during the verification stage, we propose an ndb matching algorithm based on matrix operation to accelerate the matching process of previous ndb methods caused by dictionary-based matching rules. this approach circumvents the need to store templates identical to those utilized for verification, reducing the risk of potential data leakage. extensive experiments conducted on public palmprint datasets have confirmed the effectiveness and generality of the proposed framework. upon acceptance of the paper, the code will be accessible at this https url.",2024-03-05
"87","false positive sampling-based data augmentation for enhanced 3d object detection accuracy","jiyong oh, junhaeng lee, woongchan byun, minsang kong, sang hun lee","computer vision and pattern recognition","recent studies have focused on enhancing the performance of 3d object detection models. among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. however, an inherent issue with ground-truth sampling is its tendency to increase false positives. therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3d object detection models by developing a new augmentation technique called false-positive sampling. false-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. we propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. additionally, we analyze the principles behind the performance enhancement due to false-positive sampling and propose a technique that applies the concept of curriculum learning to the sampling strategy that encompasses both false-positive and ground-truth sampling techniques. our experiments demonstrate that models utilizing false-positive sampling show a reduction in false positives and exhibit improved object detection performance. on the kitti and waymo open datasets, models with false-positive sampling surpass the baseline models by a large margin.",2024-03-05
"88","acemap: knowledge discovery through academic graph","xinbing wang, luoyi fu, xiaoying gan, ying wen, guanjie zheng, jiaxin ding, liyao xiang, nanyang ye, meng jin, shiyu liang, bin lu, haiwen wang, yi xu, cheng deng, shao zhang, huquan kang, xingli wang, qi li, zhixin guo, jiexing qi, pan liu, yuyang ren, lyuwen wu, jungang yang, jianping zhou, chenghu zhou","digital libraries","the exponential growth of scientific literature requires effective management and extraction of valuable insights. while existing scientific search engines excel at delivering search results based on relational databases, they often neglect the analysis of collaborations between scientific entities and the evolution of ideas, as well as the in-depth analysis of content within scientific publications. the representation of heterogeneous graphs and the effective measurement, analysis, and mining of such graphs pose significant challenges. to address these challenges, we present acemap, an academic system designed for knowledge discovery through academic graph. we present advanced database construction techniques to build the comprehensive acemap database with large-scale academic publications that contain rich visual, textual, and numerical information. acemap also employs innovative visualization, quantification, and analysis methods to explore associations and logical relationships among academic entities. acemap introduces large-scale academic network visualization techniques centered on nebular graphs, providing a comprehensive view of academic networks from multiple perspectives. in addition, acemap proposes a unified metric based on structural entropy to quantitatively measure the knowledge content of different academic entities. moreover, acemap provides advanced analysis capabilities, including tracing the evolution of academic ideas through citation relationships and concept co-occurrence, and generating concise summaries informed by this evolutionary process. in addition, acemap uses machine reading methods to generate potential new ideas at the intersection of different fields. exploring the integration of large language models and knowledge graphs is a promising direction for future research in idea evolution. please visit \url{this https url} for further exploration.",2024-03-05
"89","enhancing magnetocaloric material discovery: a machine learning approach using an autogenerated database by large language models","jiaoyue yuan, runqing yang, lokanath patra, bolin liao","materials science","magnetic cooling based on the magnetocaloric effect is a promising solid-state refrigeration technology for a wide range of applications in different temperature ranges. previous studies have mostly focused on near room temperature (300 k) and cryogenic temperature (< 10 k) ranges, while important applications such as hydrogen liquefaction call for efficient magnetic refrigerants for the intermediate temperature 10k to 100 k. for efficient use in this range, new magnetocaloric materials with matching curie temperatures need to be discovered, while conventional experimental approaches are typically time-consuming and expensive. here, we report a computational material discovery pipeline based on a materials database containing more than 6000 entries auto-generated by extracting reported material properties from literature using a large language model. we then use this database to train a machine learning model that can efficiently predict magnetocaloric properties of materials based on their chemical composition. we further verify the magnetocaloric properties of predicted compounds using ab initio atomistic spin dynamics simulations to close the loop for computational material discovery. using this approach, we identify 11 new promising magnetocaloric materials for the target temperature range. our work demonstrates the potential of combining large language models, machine learning, and ab initio simulations to efficiently discover new functional materials.",2024-03-05
"90","daco: towards application-driven and comprehensive data analysis via code generation","xueqing wu, rui zheng, jingzhen sha, te-lin wu, hanyu zhou, mohan tang, kai-wei chang, nanyun peng, haoran huang","computation and language","data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. in this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. however, collecting data analysis annotations curated by experts can be prohibitively expensive. we propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of llms with a multi-turn prompting technique. we construct the daco dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. we train a 6b supervised fine-tuning (sft) model on daco dataset, and find that the sft model learns reasonable data analysis capabilities. to further align the models with human preference, we use reinforcement learning to encourage generating analysis perceived by human as helpful, and design a set of dense rewards to propagate the sparse human preference reward to intermediate code generation steps. our daco-rl algorithm is evaluated by human annotators to produce more helpful answers than sft model in 57.72% cases, validating the effectiveness of our proposed algorithm. data and code are released at this https url",2024-03-04
"91","domain adaptation, explainability & fairness in ai for medical image analysis: diagnosis of covid-19 based on 3-d chest ct-scans","dimitrios kollias, anastasios arsenos, stefanos kollias","image and video processing","the paper presents the def-ai-mia cov19d competition, which is organized in the framework of the 'domain adaptation, explainability, fairness in ai for medical image analysis (def-ai-mia)' workshop of the 2024 computer vision and pattern recognition (cvpr) conference. the competition is the 4th in the series, following the first three competitions held in the framework of iccv 2021, eccv 2022 and icassp 2023 international conferences respectively. it includes two challenges on: i) covid-19 detection and ii) covid-19 domain adaptation. the competition use data from cov19-ct-db database, which is described in the paper and includes a large number of chest ct scan series. each chest ct scan series consists of a sequence of 2-d ct slices, the number of which is between 50 and 700. training, validation and test datasets have been extracted from cov19-ct-db and provided to the participants in both challenges. the paper presents the baseline models used in the challenges and the performance which was obtained respectively.",2024-03-04
"92","speech emotion recognition from voice messages recorded in the wild","lucía gómez-zaragozá, óscar valls, rocío del amor, maría josé castro-bleda, valery naranjo, mariano alcañiz raya, javier marín-morales","audio and speech processing","emotion datasets used for speech emotion recognition (ser) often contain acted or elicited speech, limiting their applicability in real-world scenarios. in this work, we used the emotional voice messages (emovome) database, including spontaneous voice messages from conversations of 100 spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. we created speaker-independent ser models using the egemaps features, transformer-based models and their combination. we compared the results with reference databases and analyzed the influence of annotators and gender fairness. the pre-trained unispeech-l model and its combination with egemaps achieved the highest results, with 61.64% and 55.57% unweighted accuracy (ua) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. for the emotion categories, 42.58% ua was obtained. emovome performed lower than the acted ravdess database. the elicited iemocap database also outperformed emovome in the prediction of emotion categories, while similar results were obtained in valence and arousal. additionally, emovome outcomes varied with annotator labels, showing superior results and better fairness when combining expert and non-expert annotations. this study significantly contributes to the evaluation of ser models in real-life situations, advancing in the development of applications for analyzing spontaneous voice messages.",2024-03-04
"93","unveiling hidden links between unseen security entities","daniel alfasi, tal shapira, anat bremler barr","cryptography and security","the proliferation of software vulnerabilities poses a significant challenge for security databases and analysts tasked with their timely identification, classification, and remediation. with the national vulnerability database (nvd) reporting an ever-increasing number of vulnerabilities, the traditional manual analysis becomes untenably time-consuming and prone to errors. this paper introduces vulnscopper, an innovative approach that utilizes multi-modal representation learning, combining knowledge graphs (kg) and natural language processing (nlp), to automate and enhance the analysis of software vulnerabilities. leveraging ultra, a knowledge graph foundation model, combined with a large language model (llm), vulnscopper effectively handles unseen entities, overcoming the limitations of previous kg approaches. we evaluate vulnscopper on two major security datasets, the nvd and the red hat cve database. our method significantly improves the link prediction accuracy between common vulnerabilities and exposures (cves), common weakness enumeration (cwes), and common platform enumerations (cpes). our results show that vulnscopper outperforms existing methods, achieving up to 78% hits@10 accuracy in linking cves to cpes and cwes and presenting an 11.7% improvement over large language models in predicting cwe labels based on the red hat database. based on the nvd, only 6.37% of the linked cpes are being published during the first 30 days; many of them are related to critical and high-risk vulnerabilities which, according to multiple compliance frameworks (such as cisa and pci), should be remediated within 15-30 days. our model can uncover new products linked to vulnerabilities, reducing remediation time and improving vulnerability management. we analyzed several cves from 2023 to showcase this ability.",2024-03-04
"94","ocel (object-centric event log) 2.0 specification","alessandro berti, istvan koren, jan niklas adams, gyunam park, benedikt knopp, nina graves, majid rafiei, lukas liß, leah tacke genannt unterberg, yisong zhang, christopher schwanen, marco pegoraro, wil m.p. van der aalst","databases","object-centric event logs (ocels) form the basis for object-centric process mining (ocpm). ocel 1.0 was first released in 2020 and triggered the development of a range of ocpm techniques. ocel 2.0 forms the new, more expressive standard, allowing for more extensive process analyses while remaining in an easily exchangeable format. in contrast to the first ocel standard, it can depict changes in objects, provide information on object relationships, and qualify these relationships to other objects or specific events. compared to xes, it is more expressive, less complicated, and better readable. ocel 2.0 offers three exchange formats: a relational database (sqlite), xml, and json format. this ocel 2.0 specification document provides an introduction to the standard, its metamodel, and its exchange formats, aimed at practitioners and researchers alike.",2024-03-04
"95","arabic text sentiment analysis: reinforcing human-performed surveys with wider topic analysis","latifah almurqren, ryan hodgson, alexandra cristea","computation and language","sentiment analysis (sa) has been, and is still, a thriving research area. however, the task of arabic sentiment analysis (asa) is still underrepresented in the body of research. this study offers the first in-depth and in-breadth analysis of existing asa studies of textual content and identifies their common themes, domains of application, methods, approaches, technologies and algorithms used. the in-depth study manually analyses 133 asa papers published in the english language between 2002 and 2020 from four academic databases (sage, ieee, springer, wiley) and from google scholar. the in-breadth study uses modern, automatic machine learning techniques, such as topic modelling and temporal analysis, on open access resources, to reinforce themes and trends identified by the prior study, on 2297 asa publications between 2010-2020. the main findings show the different approaches used for asa: machine learning, lexicon-based and hybrid approaches. other findings include asa 'winning' algorithms (svm, nb, hybrid methods). deep learning methods, such as lstm can provide higher accuracy, but for asa sometimes the corpora are not large enough to support them. additionally, whilst there are some asa corpora and lexicons, more are required. specifically, arabic tweets corpora and datasets are currently only moderately sized. moreover, arabic lexicons that have high coverage contain only modern standard arabic (msa) words, and those with arabic dialects are quite small. thus, new corpora need to be created. on the other hand, asa tools are stringently lacking. there is a need to develop asa tools that can be used in industry, as well as in academia, for arabic text sa. hence, our study offers insights into the challenges associated with asa research and provides suggestions for ways to move the field forward such as lack of dialectical arabic resource, arabic tweets, corpora and data sets for sa.",2024-03-04
"96","schema-based query optimisation for graph databases","chandan sharma (tyrex), pierre genevès (tyrex), nils gesbert (tyrex), nabil layaïda (tyrex)","databases","recursive graph queries are increasingly popular for extracting information from interconnected data found in various domains such as social networks, life sciences, and business analytics. graph data often come with schema information that describe how nodes and edges are organized. we propose a type inference mechanism that enriches recursive graph queries with relevant structural information contained in a graph schema. we show that this schema information can be useful in order to improve the performance when evaluating acylic recursive graph queries. furthermore, we prove that the proposed method is sound and complete, ensuring that the semantics of the query is preserved during the schema-enrichment process.",2024-03-04
"97","relational to rdf data migration by query co-evaluation","ryan wisnesky, daniel filonik","databases","in this paper we define a new algorithm to convert an input relational database to an output set of rdf triples. the algorithm can be used to e.g. load csv data into a financial owl ontology such as fibo. the algorithm takes as input a set of relational conjunctive (select-from-where) queries, one for each input table, from the three column (subject, predicate, object) output rdf schema to the input table's relational schema. the algorithm's output is the only set of rdf triples for which a unique round-trip of the input data under the relational queries exists. the output may contain blank nodes, is unique up to unique isomorphism, and can be obtained using elementary formal methods (equational theorem proving and term model construction specifically). we also describe how (generalized) homomorphisms between graphs can be used to write such relational conjunctive (select-from-where) queries, which, due to the lack of structure in the three-column rdf schema, tend to be large in practice. we demonstrate examples of both the algorithm and mapping language on the fibo financial ontology.",2024-03-03
"98","respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy","michel pohl, mitsuru uesaka, hiroyuki takahashi, kazuyuki demachi, ritu bhusal chhatkuli","machine learning","in lung radiotherapy, infrared cameras can record the location of reflective objects on the chest to infer the position of the tumor moving due to breathing, but treatment system latencies hinder radiation beam precision. real-time recurrent learning (rtrl), is a potential solution as it can learn patterns within non-stationary respiratory data but has high complexity. this study assesses the capabilities of resource-efficient online rnn algorithms, namely unbiased online recurrent optimization (uoro), sparse-1 step approximation (snap-1), and decoupled neural interfaces (dni) to forecast respiratory motion during radiotherapy treatment accurately. we use time series containing the 3d position of external markers on the chest of healthy subjects. we propose efficient implementations for snap-1 and dni based on compression of the influence and immediate jacobian matrices and an accurate update of the linear coefficients used in credit assignment estimation, respectively. the original sampling frequency was 10hz; we performed resampling at 3.33hz and 30hz. we use uoro, snap-1, and dni to forecast each marker's 3d position with horizons (the time interval in advance for which the prediction is made) h<=2.1s and compare them with rtrl, least mean squares, and linear regression. rnns trained online achieved similar or better accuracy than most previous works using larger training databases and deep learning, even though we used only the first minute of each sequence to predict motion within that exact sequence. snap-1 had the lowest normalized root mean square errors (nrmse) averaged over the horizon values considered, equal to 0.335 and 0.157, at 3.33hz and 10.0hz, respectively. similarly, uoro had the highest accuracy at 30hz, with an nrmse of 0.0897. dni's inference time, equal to 6.8ms per time step at 30hz (intel core i7-13700 cpu), was the lowest among the rnn methods examined.",2024-03-03
"99","approximations and hardness of packing partially ordered items","ilan doron-arad, guy kortsarz, joseph naor, baruch schieber, hadas shachnai","data structures and algorithms","motivated by applications in production planning and storage allocation in hierarchical databases, we initiate the study of covering partially ordered items (cpo). given a capacity $k \in \mathbb{z}^+$, and a directed graph $g=(v,e)$ where each vertex has a size in $\{0,1, \ldots,k\}$, we seek a collection of subsets of vertices $s_1, \ldots, s_m$ that cover all the vertices, such that for any $1 \leq j \leq m$, the total size of vertices in $s_j$ is bounded by $k$, and there are no edges from $v \setminus s_j$ to $s_j$. the objective is to minimize the number of subsets $m$. cpo is closely related to the rule caching problem (rcp) that is of wide interest in the networking area. the input for rcp is a directed graph $g=(v,e)$, a profit function $p:v \rightarrow \mathbb{z}_{0}^+$, and $k \in \mathbb{z}^+$. the output is a subset $s \subseteq v$ of maximum profit such that $|s| \leq k$ and there are no edges from $v \setminus s$ to $s$. our main result is a $2$-approximation algorithm for cpo on out-trees, complemented by an asymptotic $1.5$-hardness of approximation result. we also give a two-way reduction between rcp and the densest $k$-subhypergraph problem, surprisingly showing that the problems are equivalent w.r.t. polynomial-time approximation within any factor $\rho \geq 1$. this implies that rcp cannot be approximated within factor $|v|^{1-\eps}$ for any fixed $\eps>0$, under standard complexity assumptions. prior to this work, rcp was just known to be strongly np-hard. we further show that there is no eptas for the special case of rcp where the profits are uniform, assuming gap-eth. since this variant admits a ptas, we essentially resolve the complexity status of this problem.",2024-03-03
"100","rematch: retrieval enhanced schema matching with llms","eitam sheetrit, menachem brief, moshik mishaeli, oren elisha","databases","schema matching is a crucial task in data integration, involving the alignment of a source database schema with a target schema to establish correspondence between their elements. this task is challenging due to textual and semantic heterogeneity, as well as differences in schema sizes. although machine-learning-based solutions have been explored in numerous studies, they often suffer from low accuracy, require manual mapping of the schemas for model training, or need access to source schema data which might be unavailable due to privacy concerns. in this paper we present a novel method, named rematch, for matching schemas using retrieval-enhanced large language models (llms). our method avoids the need for predefined mapping, any model training, or access to data in the source database. in the rematch method the tables of the target schema and the attributes of the source schema are first represented as structured passage-based documents. for each source attribute document, we retrieve $j$ documents, representing target schema tables, according to their semantic relevance. subsequently, we create a prompt for every source table, comprising all its attributes and their descriptions, alongside all attributes from the set of top $j$ target tables retrieved previously. we employ llms using this prompt for the matching task, yielding a ranked list of $k$ potential matches for each source attribute. our experimental results on large real-world schemas demonstrate that rematch significantly improves matching capabilities and outperforms other machine learning approaches. by eliminating the requirement for training data, rematch becomes a viable solution for real-world scenarios.",2024-03-03
"101","spectral operator representations","austin zadoks, antimo marrazzo, nicola marzari","materials science","machine learning in atomistic materials science has grown to become a powerful tool, with most approaches focusing on atomic arrangements, typically decomposed into local atomic environments. this approach, while well-suited for machine-learned interatomic potentials, is conceptually at odds with learning complex intrinsic properties of materials, often driven by spectral properties commonly represented in reciprocal space (e.g., band gaps or mobilities) which cannot be readily atomically partitioned. for such applications, methods which represent the electronic rather than the atomic structure could be more promising. in this work, we present a general framework focused on electronic-structure descriptors which take advantage of the natural symmetries and inherent interpretability of physical models. using this framework, we formulate two such representations and apply them respectively to measuring the similarity of carbon nanotubes and barium titanate polymorphs, and to the discovery of novel transparent conducting materials (tcms) in the materials cloud 3d database (mc3d). a random forest classifier trained on 1% of the materials in the mc3d is able to correctly label 76% of entries in database which meet common screening criteria for promising tcms.",2024-03-03
"102","applying self-supervised learning to network intrusion detection for network flows with graph neural network","renjie xu, guangwei wu, weiping wang, xing gao, an he, zhengpeng zhang","machine learning","graph neural networks (gnns) have garnered intensive attention for network intrusion detection system (nids) due to their suitability for representing the network traffic flows. however, most present gnn-based methods for nids are supervised or semi-supervised. network flows need to be manually annotated as supervisory labels, a process that is time-consuming or even impossible, making nids difficult to adapt to potentially complex attacks, especially in large-scale real-world scenarios. the existing gnn-based self-supervised methods focus on the binary classification of network flow as benign or not, and thus fail to reveal the types of attack in practice. this paper studies the application of gnns to identify the specific types of network flows in an unsupervised manner. we first design an encoder to obtain graph embedding, that introduces the graph attention mechanism and considers the edge information as the only essential factor. then, a self-supervised method based on graph contrastive learning is proposed. the method samples center nodes, and for each center node, generates subgraph by it and its direct neighbor nodes, and corresponding contrastive subgraph from the interpolated graph, and finally constructs positive and negative samples from subgraphs. furthermore, a structured contrastive loss function based on edge features and graph local topology is introduced. to the best of our knowledge, it is the first gnn-based self-supervised method for the multiclass classification of network flows in nids. detailed experiments conducted on four real-world databases (nf-bot-iot, nf-bot-iot-v2, nf-cse-cic-ids2018, and nf-cse-cic-ids2018-v2) systematically compare our model with the state-of-the-art supervised and self-supervised models, illustrating the considerable potential of our method. our code is accessible through this https url.",2024-03-03
"103","enhancing data provenance and model transparency in federated learning systems -- a database approach","michael gu, ramasoumya naraparaju, dongfang zhao","cryptography and security","federated learning (fl) presents a promising paradigm for training machine learning models across decentralized edge devices while preserving data privacy. ensuring the integrity and traceability of data across these distributed environments, however, remains a critical challenge. the ability to create transparent artificial intelligence, such as detailing the training process of a machine learning model, has become an increasingly prominent concern due to the large number of sensitive (hyper)parameters it utilizes; thus, it is imperative to strike a reasonable balance between openness and the need to protect sensitive information. in this paper, we propose one of the first approaches to enhance data provenance and model transparency in federated learning systems. our methodology leverages a combination of cryptographic techniques and efficient model management to track the transformation of data throughout the fl process, and seeks to increase the reproducibility and trustworthiness of a trained fl model. we demonstrate the effectiveness of our approach through experimental evaluations on diverse fl scenarios, showcasing its ability to tackle accountability and explainability across the board. our findings show that our system can greatly enhance data transparency in various fl environments by storing chained cryptographic hashes and client model snapshots in our proposed design for data decoupled fl. this is made possible by also employing multiple optimization techniques which enables comprehensive data provenance without imposing substantial computational loads. extensive experimental results suggest that integrating a database subsystem into federated learning systems can improve data provenance in an efficient manner, encouraging secure fl adoption in privacy-sensitive applications and paving the way for future advancements in fl transparency and security features.",2024-03-03
"104","temperature, pressure, velocity, and water vapor mole fraction profiles in a ramjet combustor using dual frequency comb spectroscopy and a high temperature absorption database","david yun, scott c. egbert, nathan a. malarich, ryan k. cole, jacob j. france, jiwen liu, kristin m. rice, mark a. hagenmaier, jeffrey m. donbar, nazanin hoghooghi, sean c. coburn, gregory b. rieker","fluid dynamics","accurate diagnostics of the combustor region of ramjet engines can improve engine design and create benchmarks for computational fluid dynamics models. previous works demonstrate that dual frequency comb spectroscopy can provide low uncertainty diagnostics of multiple flow parameters in the non-combusting regions of ramjets. however, the high temperatures present in the combustor present a challenge for broadband spectroscopic absorption models that are used to interpret measurements in these regions. here, we utilize a new water vapor absorption database created for high temperature water-air mixtures to fit spectra measured in a ground-test ramjet engine with a broadband near-infrared dual comb absorption spectrometer. we extract 2d profiles of pressure, temperature, water mole fraction, and velocity using this new database. we demonstrate that the new database provides the lowest fit residuals compared to other water vapor absorption databases. we compare computational fluid dynamics simulations of the combustor with the measured data to demonstrate that the simulations overpredict heat release and water vapor production.",2024-03-03
"105","single-beam velocimetry with dual frequency comb absorption spectroscopy","david yun, scott c. egbert, augustine t. frymire, sean c. coburn, jacob j. france, kristin m. rice, jeffrey m. donbar, gregory b. rieker","optics","laser absorption doppler velocimeters use a crossed-beam configuration to cancel error due to laser frequency drift and absorption model uncertainty. this configuration complicates the spatial interpretation of the measurement since the two beams sample different volumes of gas. here, we achieve single-beam velocimetry with a portable dual comb spectrometer (dcs) with high frequency accuracy and stability enabled by gps-referencing, and a new high-temperature water vapor absorption database. we measure the inlet flow in a supersonic ramjet engine and demonstrate single-beam measurements that are on average within 19 m/s of concurrent crossed-beam measurements. we estimate that the dcs and the new database contribute 1.6 and 13 m/s to this difference respectively.",2024-03-03
"106","d-dse: distinct dynamic searchable encryption resisting volume leakage in encrypted databases","dongli liu, wei wang, peng xu, laurence t. yang, bo luo, kaitai liang","cryptography and security","dynamic searchable encryption (dse) has emerged as a solution to efficiently handle and protect large-scale data storage in encrypted databases (edbs). volume leakage poses a significant threat, as it enables adversaries to reconstruct search queries and potentially compromise the security and privacy of data. padding strategies are common countermeasures for the leakage, but they significantly increase storage and communication costs. in this work, we develop a new perspective to handle volume leakage. we start with distinct search and further explore a new concept called \textit{distinct} dse (\textit{d}-dse). we also define new security notions, in particular distinct with volume-hiding security, as well as forward and backward privacy, for the new concept. based on \textit{d}-dse, we construct the \textit{d}-dse designed edb with related constructions for distinct keyword (d-kw-\textit{d}dse), keyword (kw-\textit{d}dse), and join queries (join-\textit{d}dse) and update queries in encrypted databases. we instantiate a concrete scheme \textsf{bf-sre}, employing symmetric revocable encryption. we conduct extensive experiments on real-world datasets, such as crime, wikipedia, and enron, for performance evaluation. the results demonstrate that our scheme is practical in data search and with comparable computational performance to the sota dse scheme (\textsf{mitra}*, \textsf{aura}) and padding strategies (\textsf{seal}, \textsf{shielddb}). furthermore, our proposal sharply reduces the communication cost as compared to padding strategies, with roughly 6.36 to 53.14x advantage for search queries.",2024-03-02
"107","query recovery from easy to hard: jigsaw attack against sse","hao nie, wei wang, peng xu, xianglong zhang, laurence t. yang, kaitai liang","cryptography and security","searchable symmetric encryption schemes often unintentionally disclose certain sensitive information, such as access, volume, and search patterns. attackers can exploit such leakages and other available knowledge related to the user's database to recover queries. we find that the effectiveness of query recovery attacks depends on the volume/frequency distribution of keywords. queries containing keywords with high volumes/frequencies are more susceptible to recovery, even when countermeasures are implemented. attackers can also effectively leverage these ``special'' queries to recover all others. by exploiting the above finding, we propose a jigsaw attack that begins by accurately identifying and recovering those distinctive queries. leveraging the volume, frequency, and co-occurrence information, our attack achieves $90\%$ accuracy in three tested datasets, which is comparable to previous attacks (oya et al., usenix' 22 and damie et al., usenix' 21). with the same runtime, our attack demonstrates an advantage over the attack proposed by oya et al (approximately $15\%$ more accuracy when the keyword universe size is 15k). furthermore, our proposed attack outperforms existing attacks against widely studied countermeasures, achieving roughly $60\%$ and $85\%$ accuracy against the padding and the obfuscation, respectively. in this context, with a large keyword universe ($\geq$3k), it surpasses current state-of-the-art attacks by more than $20\%$.",2024-03-02
"108","dfin-sql: integrating focused schema with din-sql for superior accuracy in large-scale databases","shai volvovsky, marco marcassa, mustafa panbiharwala","databases","the task of converting natural language queries into sql queries is intricate, necessitating a blend of precise techniques for an accurate translation. the din-sql (decomposed-in-context sql) methodology represents a significant development in this domain. this paper introduces dfin (decomposed focused-in-context), an innovative extension of din-sql that enhances text-to-sql conversion by addressing schema linking errors, which are a major source of inaccuracies. dfin uniquely alternates between prompting techniques and retrieval-augmented generation (rag), adapting to the size and complexity of the database schema. a preprocessing phase embeds database definitions and leverages annotated files, akin to those in the bird dataset, facilitating the runtime retrieval of pertinent schema information. this strategy significantly reduces the token count for schema linking prompts, enabling the use of a standard gpt-4 model over its larger context variant, thus handling large-scale databases more effectively and economically. our evaluation on the bird dataset, a challenging real-world benchmark, demonstrates that dfin not only scales efficiently but also improves accuracy, achieving a score of 51.69. this improvement surpasses din-sql method (the current third-place), which is the highest-ranked model employing in-context learning rather than fine-tuning, previously scoring 50.72. the advancement of dfin underscores the evolving capabilities of in-context learning methodologies combined with advanced language models, offering a promising avenue for future research in complex text-to-sql conversion tasks.",2024-03-01
"109","retrieval augmented generation systems: automatic dataset creation, evaluation and boolean agent setup","tristan kenneweg, philip kenneweg, barbara hammer","information retrieval","retrieval augmented generation (rag) systems have seen huge popularity in augmenting large-language model (llm) outputs with domain specific and time sensitive data. very recently a shift is happening from simple rag setups that query a vector database for additional information with every user input to more sophisticated forms of rag. however, different concrete approaches compete on mostly anecdotal evidence at the moment. in this paper we present a rigorous dataset creation and evaluation workflow to quantitatively compare different rag strategies. we use a dataset created this way for the development and evaluation of a boolean agent rag setup: a system in which a llm can decide whether to query a vector database or not, thus saving tokens on questions that can be answered with internal knowledge. we publish our code and generated dataset online.",2024-02-26
"110","$\textit{l+m-24}$: building a dataset for language + molecules @ acl 2024","carl edwards, qingyun wang, lawrence zhao, heng ji","computation and language","language-molecule models have emerged as an exciting direction for molecular discovery and understanding. however, training these models is challenging due to the scarcity of molecule-language pair datasets. at this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. in this document, we detail the $\textit{l+m-24}$ dataset, which has been created for the language + molecules workshop shared task at acl 2024. in particular, $\textit{l+m-24}$ is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction.",2024-02-22
"111","colon: the largest colonoscopy long sequence public database","lina ruiz, franklin sierra-jerez, jair ruiz, fabio martinez","computer vision and pattern recognition","colorectal cancer is the third most aggressive cancer worldwide. polyps, as the main biomarker of the disease, are detected, localized, and characterized through colonoscopy procedures. nonetheless, during the examination, up to 25% of polyps are missed, because of challenging conditions (camera movements, lighting changes), and the close similarity of polyps and intestinal folds. besides, there is a remarked subjectivity and expert dependency to observe and detect abnormal regions along the intestinal tract. currently, publicly available polyp datasets have allowed significant advances in computational strategies dedicated to characterizing non-parametric polyp shapes. these computational strategies have achieved remarkable scores of up to 90% in segmentation tasks. nonetheless, these strategies operate on cropped and expert-selected frames that always observe polyps. in consequence, these computational approximations are far from clinical scenarios and real applications, where colonoscopies are redundant on intestinal background with high textural variability. in fact, the polyps typically represent less than 1% of total observations in a complete colonoscopy record. this work introduces colon: the largest colonoscopy long sequence dataset with around of 30 thousand polyp labeled frames and 400 thousand background frames. the dataset was collected from a total of 30 complete colonoscopies with polyps at different stages, variations in preparation procedures, and some cases the observation of surgical instrumentation. additionally, 10 full intestinal background video control colonoscopies were integrated in order to achieve a robust polyp-background frame differentiation. the colon dataset is open to the scientific community to bring new scenarios to propose computational tools dedicated to polyp detection and segmentation over long sequences, being closer to real colonoscopy scenarios.",2024-03-01
"112","modelling global fossil co2 emissions with a lognormal distribution: a climate policy tool","faustino prieto, catalina b. garcía-garcía, román salmerón gómez","general economics","carbon dioxide (co2) emissions have emerged as a critical issue with profound impacts on the environment, human health, and the global economy. the steady increase in atmospheric co2 levels, largely due to human activities such as burning fossil fuels and deforestation, has become a major contributor to climate change and its associated catastrophic effects. to tackle this pressing challenge, a coordinated global effort is needed, which necessitates a deep understanding of emissions patterns and trends. in this paper, we explore the use of statistical modelling, specifically the lognormal distribution, as a framework for comprehending and predicting co2 emissions. we build on prior research that suggests a complex distribution of emissions and seek to test the hypothesis that a simpler distribution can still offer meaningful insights for policy-makers. we utilize data from three comprehensive databases and analyse six candidate distributions (exponential, fisk, gamma, lognormal, lomax, weibull) to identify a suitable model for global fossil co2 emissions. our findings highlight the adequacy of the lognormal distribution in characterizing emissions across all countries and years studied. furthermore, to provide additional support for this distribution, we provide statistical evidence supporting the applicability of gibrat's law to those co2 emissions. finally, we employ the lognormal model to predict emission parameters for the coming years and propose two policies for reducing total fossil co2 emissions. our research aims to provide policy-makers with accurate and detailed information to support effective climate change mitigation strategies.",2024-03-01
"113","towards localized accuracy assessment of remote-sensing derived built-up land layers across the rural-urban continuum","johannes h. uhl, stefan leyk","physics and society","the accuracy assessment of remote-sensing derived built-up land data represents a specific case of binary map comparison, where class imbalance varies considerably across rural-urban trajectories. thus, local accuracy characterization of such datasets requires specific strategies that are robust to low sample sizes and different levels of class imbalance. herein, we examine the suitability of commonly used spatial agreement measures for their localized accuracy characterization of built-up land layers across the rural-urban continuum, using the global human settlement layer and a reference database of built-up land derived from cadastral and building footprint data.",2024-02-29
"114","rinalmo: general-purpose rna language models can generalize well on structure prediction tasks","rafael josip penić, tin vlašić, roland g. huber, yue wan, mile šikić","biomolecules","ribonucleic acid (rna) plays a variety of crucial roles in fundamental biological processes. recently, rna has become an interesting drug target, emphasizing the need to improve our understanding of its structures and functions. over the years, sequencing technologies have produced an enormous amount of unlabeled rna data, which hides important knowledge and potential. motivated by the successes of protein language models, we introduce ribonucleic acid language model (rinalmo) to help unveil the hidden code of rna. rinalmo is the largest rna language model to date with $650$ million parameters pre-trained on $36$ million non-coding rna sequences from several available databases. rinalmo is able to extract hidden knowledge and capture the underlying structure information implicitly embedded within the rna sequences. rinalmo achieves state-of-the-art results on several downstream tasks. notably, we show that its generalization capabilities can overcome the inability of other deep learning methods for secondary structure prediction to generalize on unseen rna families. the code has been made publicly available on this https url.",2024-02-29
"115","from design to device: challenges and opportunities in computational discovery of p-type transparent conductors","rachel woods-robinson, monica morales-masis, geoffroy hautier, andrea crovetto","applied physics","a high-performance p-type transparent conductor (tc) does not yet exist, but could lead to advances in a wide range of optoelectronic applications and enable new architectures for, e.g., next-generation photovoltaic (pv) devices. high-throughput computational material screenings have been a promising approach to filter databases and identify new p-type tc candidates, and some of these predictions have been experimentally validated. however, most of these predicted candidates do not have experimentally-achieved properties on par with n-type tcs used in solar cells, and therefore have not yet been used in commercial devices. thus, there is still a significant divide between transforming predictions into results that are actually achievable in the lab, and an even greater lag in scaling predicted materials into functional devices. in this perspective, we outline some of the major disconnects in this materials discovery process -- from scaling computational predictions into synthesizable crystals and thin films in the laboratory, to scaling lab-grown films into real-world solar devices -- and share insights to inform future strategies for tc discovery and design.",2024-02-29
"116","modular blind video quality assessment","wen wen, mu li, yabin zhang, yiting liao, junlin li, li zhang, kede ma","image and video processing","blind video quality assessment (bvqa) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services. contemporary deep learning-based models primarily analyze the video content in its aggressively downsampled format, while being blind to the impact of actual spatial resolution and frame rate on video quality. in this paper, we propose a modular bvqa model, and a method of training it to improve its modularity. specifically, our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively. during training, spatial and temporal rectifiers are dropped out with some probabilities so as to make the base quality predictor a standalone bvqa model, which should work better with the rectifiers. extensive experiments on both professionally-generated content and user generated content video databases show that our quality model achieves superior or comparable performance to current methods. furthermore, the modularity of our model offers a great opportunity to analyze existing video quality databases in terms of their spatial and temporal complexities. last, our bvqa model is cost-effective to add other quality-relevant video attributes such as dynamic range and color gamut as additional rectifiers.",2024-02-29
"117","plangpt: enhancing urban planning with tailored language model and efficient retrieval","he zhu, wenjia zhang, nuoxian huang, boyang li, luyao niu, zipei fan, tianle lun, yicheng tao, junyou su, zhaoya gong, chenyu fang, xing liu","computation and language","in the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. to enhance the efficiency of urban professionals and overcome these obstacles, we introduce plangpt, the first specialized large language model tailored for urban and spatial planning. developed through collaborative efforts with institutions like the chinese academy of urban planning, plangpt leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. empirical tests demonstrate that plangpt has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.",2024-02-29
"118","a sound approach: using large language models to generate audio descriptions for egocentric text-audio retrieval","andreea-maria oncescu, joão f. henriques, andrew zisserman, samuel albanie, a. sophia koepke","audio and speech processing","video databases from the internet are a valuable source of text-audio retrieval datasets. however, given that sound and vision streams represent different ""views"" of the data, treating visual descriptions as audio descriptions is far from optimal. even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval. to exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using large language models (llms). in this work, we consider the egocentric video setting and propose three new text-audio retrieval benchmarks based on the epicmir and egomcq tasks, and on the epicsounds dataset. our approach for obtaining audio-centric descriptions gives significantly higher zero-shot performance than using the original visual-centric descriptions. furthermore, we show that using the same prompts, we can successfully employ llms to improve the retrieval on epicsounds, compared to using the original audio class labels of the dataset. finally, we confirm that llms can be used to determine the difficulty of identifying the action associated with a sound.",2024-02-29
"119","hunting for exocomet transits in the tess database using the random forest method","d.v. dobrycheva, m.yu. vasylenko, i.v. kulyk, ya.v. pavlenko, o.s. shubina, i.v. luk'yanyk, p.p. korsun","earth and planetary astrophysics","this study introduces an approach to detecting exocomet transits in the dataset of the transiting exoplanet survey satellite (tess), specifically within its sector 1. given the limited number of exocomet transits detected in the observed light curves, creating a sufficient training sample for the machine learning method was challenging. we developed a unique training sample by encapsulating simulated asymmetric transit profiles into observed light curves, thereby creating realistic data for the model training. to analyze these light curves, we employed the tsfresh software, which was a tool for extracting key features that were then used to refine our random forest model training. considering that cometary transits typically exhibit a small depth, less than 1% of the star's brightness, we chose to limit our sample to the cdpp parameter. our study focused on two target samples: light curves with a cdpp of less than 40 ppm and light curves with a cdpp of up to 150 ppm. each sample was accompanied by a corresponding training set. this methodology achieved an accuracy of approximately 96%, with both precision and recall rates exceeding 95% and a balanced f1-score of around 96%. this level of accuracy was effective in distinguishing between 'exocomet candidate' and 'non-candidate' classifications for light curves with a cdpp of less than 40 ppm, and our model identified 12 potential exocomet candidates. however, when applying machine learning to less accurate light curves (cdpp up to 150 ppm), we noticed a significant increase in curves that could not be confidently classified, but even in this case, our model identified 20 potential exocomet candidates. these promising results within sector 1 motivate us to extend our analysis across all tess sectors to detect and study comet-like activity in the extrasolar planetary systems.",2024-02-29
"120","hyperfednet: communication-efficient personalized federated learning via hypernetwork","xingyun chen, yan huang, zhenzhen xie, junjie pang","networking and internet architecture","in response to the challenges posed by non-independent and identically distributed (non-iid) data and the escalating threat of privacy attacks in federated learning (fl), we introduce hyperfednet (hfn), a novel architecture that incorporates hypernetworks to revolutionize parameter aggregation and transmission in fl. traditional fl approaches, characterized by the transmission of extensive parameters, not only incur significant communication overhead but also present vulnerabilities to privacy breaches through gradient analysis. hfn addresses these issues by transmitting a concise set of hypernetwork parameters, thereby reducing communication costs and enhancing privacy protection. upon deployment, the hfn algorithm enables the dynamic generation of parameters for the basic layer of the fl main network, utilizing local database features quantified by embedding vectors as input. through extensive experimentation, hfn demonstrates superior performance in reducing communication overhead and improving model accuracy compared to conventional fl methods. by integrating the hfn algorithm into the fl framework, hfn offers a solution to the challenges of non-iid data and privacy threats.",2024-02-28
"121","formalized identification of key factors in safety-relevant failure scenarios","tim maurice julitz, nadine schlüter, manuel löwer","software engineering","this research article presents a methodical data-based approach to systematically identify key factors in safety-related failure scenarios, with a focus on complex product-environmental systems in the era of industry 4.0. the study addresses the uncertainty arising from the growing complexity of modern products. the method uses scenario analysis and focuses on failure analysis within technical product development. the approach involves a derivation of influencing factors based on information from failure databases. the failures described here are documented individually in failure sequence diagrams and then related to each other in a relationship matrix. this creates a network of possible failure scenarios from individual failure cases that can be used in product development. to illustrate the application of the methodology, a case study of 41 rapex safety alerts for a hair dryer is presented. the failure sequence diagrams and influencing factor relationship matrices show 46 influencing factors that lead to safety-related failures. the predominant harm is burns and electric shocks, which are highlighted by the active and passive sum diagrams. the research demonstrates a robust method for identifying key factors in safety-related failure scenarios using information from failure databases. the methodology provides valuable insights into product development and emphasizes the frequency of influencing factors and their interconnectedness.",2024-02-28
"122","water-vapor absorption database using dual comb spectroscopy from 300-1300 k part ii: air-broadened h$_2$o, 6600 to 7650 cm$^{-1}$","scott c. egbert, keeyoon sung, sean c. coburn, brian j. drouin, gregory b. rieker","atmospheric and oceanic physics","we present broadband dual frequency comb laser absorption measurements of 2% h$_2$o (natural isotopic abundance of 99.7% h$_2^{16}$o) in air from 6600-7650 cm$^{-1}$ (1307-1515 nm) with a spectral point spacing of 0.0068 cm$^{-1}$. twenty-nine datasets were collected at temperatures between 300 and 1300 k ($\pm$0.82% average uncertainty) and pressures ranging from 20 to 600 torr ($\pm$0.25%) with an average residual absorbance noise of 8.0e-4 across the spectrum for all measurements. we fit measurements using a quadratic speed-dependent voigt profile to determine 7088 absorption parameters for 3366 individual transitions found in hitran2020. these measurements build on the line strength, line center, self-broadening, and self-shift parameters determined in the part i companion of this work. here we measure air-broadened width (with temperature- and speed-dependence) and air pressure shift (with temperature dependence) parameters. various trends are explored for extrapolation to weak transitions that were not covered in this work. improvements made in this work are predominantly due to the inclusion of air pressure shift temperature dependence values. in aggregate, these updates improved rms absorbance error by a factor of 4.2 on average, and the remaining residual is predominantly spectral noise. this updated database improves high temperature spectroscopic knowledge across the 6600 7650 cm$^{-1}$ region of h$_2$o absorption.",2024-02-28
"123","hy-dat: a tool to address hydropower modeling gaps using interdependency, efficiency curves, and unit dispatch models","dewei wang, bhaskar mitra, sameer nekkalapu, sohom datta, bibi matthew, rounak meyur, heng wang, slaven kincic","signal processing","as the power system continues to be flooded with intermittent resources, it becomes more important to accurately assess the role of hydro and its impact on the power grid. while hydropower generation has been studied for decades, dependency of power generation on water availability and constraints in hydro operation are not well represented in power system models used in the planning and operation of large-scale interconnection studies. there are still multiple modeling gaps that need to be addressed; if not, they can lead to inaccurate operation and planning reliability studies, and consequently to unintentional load shedding or even blackouts. as a result, it is very important that hydropower is represented correctly in both steady-state and dynamic power system studies. in this paper, we discuss the development and use of the hydrological dispatch and analysis tool (hy-dat) as an interactive graphical user interface, that uses a novel methodology to address the hydropower modeling gaps like water availability and interdependency using a database and algorithms to generate accurate representative models for power system simulation.",2024-02-28
"124","physics-informed machine learning for seismic response prediction of nonlinear steel moment resisting frame structures","r. bailey bond, pu ren, jerome f. hajjar, hao sun","applied physics","there is a growing interest in utilizing machine learning (ml) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations. the existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. to address these challenges, this paper presents a novel physics-informed machine learning (piml) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. the basic concept is to constrain the solution space of the ml model within known physical bounds. this is made possible with three main features, namely, model order reduction, a long short-term memory (lstm) networks, and newton's second law (e.g., the equation of motion). model order reduction is essential for handling structural systems with inherent redundancy and enhancing model efficiency. the lstm network captures temporal dependencies, enabling accurate prediction of time series responses. the equation of motion is manipulated to learn system nonlinearities and confines the solution space within physically interpretable results. these features enable model training with relatively sparse data and offer benefits in terms of accuracy, interpretability, and robustness. furthermore, a dataset of seismically designed archetype ductile planar steel moment resistant frames under horizontal seismic loading, available in the designsafe-ci database, is considered for evaluation of the proposed method. the resulting metamodel is capable of handling more complex data compared to existing physics-guided lstm models and outperforms other non-physics data-driven neural networks.",2024-02-28
"125","fast buffet onset prediction and optimization method based on a pre-trained flowfield prediction model","yunjia yang, runze li, yufei zhang, haixin chen","fluid dynamics","the transonic buffet is a detrimental phenomenon occurs on supercritical airfoils and limits aircraft's operating envelope. traditional methods for predicting buffet onset rely on multiple computational fluid dynamics simulations to assess a series of airfoil flowfields and then apply criteria to them, which is slow and hinders optimization efforts. this article introduces an innovative approach for rapid buffet onset prediction. a machine-learning flowfield prediction model is pre-trained on a large database and then deployed offline to replace simulations in the buffet prediction process for new airfoil designs. unlike using a model to directly predict buffet onset, the proposed technique offers better visualization capabilities by providing users with intuitive flowfield outputs. it also demonstrates superior generalization ability, evidenced by a 32.5% reduction in average buffet onset prediction error on the testing dataset. the method is utilized to optimize the buffet performance of 11 distinct airfoils within and outside the training dataset. the optimization results are verified with simulations and proved to yield improved samples across all cases. it is affirmed the pre-trained flowfield prediction model can be applied to accelerate aerodynamic shape optimization, while further work still needs to raise its reliability for this safety-critical task.",2024-02-27
"126","towards spatiotemporal integration of bus transit with data-driven approaches","júlio borges, altieris m. peixoto, thiago h. silva, anelise munaretto, ricardo luders","social and information networks","this study aims to propose an approach for spatiotemporal integration of bus transit, which enables users to change bus lines by paying a single fare. this could increase bus transit efficiency and, consequently, help to make this mode of transportation more attractive. usually, this strategy is allowed for a few hours in a non-restricted area; thus, certain walking distance areas behave like ""virtual terminals."" for that, two data-driven algorithms are proposed in this work. first, a new algorithm for detecting itineraries based on bus gps data and the bus stop location. the proposed algorithm's results show that 90% of the database detected valid itineraries by excluding invalid markings and adding times at missing bus stops through temporal interpolation. second, this study proposes a bus stop clustering algorithm to define suitable areas for these virtual terminals where it would be possible to make bus transfers outside the physical terminals. using real-world origin-destination trips, the bus network, including clusters, can reduce traveled distances by up to 50%, making twice as many connections on average.",2024-02-27
"127","exploring gene regulatory interaction networks and predicting therapeutic molecules for hypopharyngeal cancer and egfr-mutated lung adenocarcinoma","abanti bhattacharjya, md manowarul islam, md ashraf uddin, md. alamin talukder, akm azad, sunil aryal, bikash kumar paul, wahia tasnim, muhammad ali abdulllah almoyad, mohammad ali moni","genomics","with the advent of information technology, the bioinformatics research field is becoming increasingly attractive to researchers and academicians. the recent development of various bioinformatics toolkits has facilitated the rapid processing and analysis of vast quantities of biological data for human perception. most studies focus on locating two connected diseases and making some observations to construct diverse gene regulatory interaction networks, a forerunner to general drug design for curing illness. for instance, hypopharyngeal cancer is a disease that is associated with egfr-mutated lung adenocarcinoma. in this study, we select egfr-mutated lung adenocarcinoma and hypopharyngeal cancer by finding the lung metastases in hypopharyngeal cancer. to conduct this study, we collect mircorarray datasets from geo (gene expression omnibus), an online database controlled by ncbi. differentially expressed genes, common genes, and hub genes between the selected two diseases are detected for the succeeding move. our research findings have suggested common therapeutic molecules for the selected diseases based on 10 hub genes with the highest interactions according to the degree topology method and the maximum clique centrality (mcc). our suggested therapeutic molecules will be fruitful for patients with those two diseases simultaneously.",2024-02-27
"128","wavelet scattering transform for bioacustics: application to watkins marine mammal sound database","davide carbone (1 and 2), alessandro licciardi (1 and 2) ((1) politecnico di torino, (2) istituto nazionale di fisica nucleare sezione di torino)","signal processing","marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. the watkins marine mammal sound database (wmmd) is an extensive labeled dataset used in machine learning applications. however, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. this study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. subsequently, we propose the application of the wavelet scattering transform (wst) in place of standard methods based on the short-time fourier transform (stft). the study also tackles a classification task using an ad-hoc deep architecture with residual layers. we outperform the existing classification architecture by $6\%$ in accuracy using wst and $8\%$ using mel spectrogram preprocessing, effectively reducing by half the number of misclassified samples, and reaching a top accuracy of $96\%$.",2024-02-20
"129","emotional voice messages (emovome) database: emotion recognition in spontaneous voice messages","lucía gómez zaragozá (1), rocío del amor (1), elena parra vargas (1), valery naranjo (1), mariano alcañiz raya (1), javier marín-morales (1) ((1) human-tech institute, universitat politènica de valència, valencia, spain)","sound","emotional voice messages (emovome) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 spanish speakers, gender balanced. voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. the experts also provided an extra label corresponding to seven emotion categories. to set a baseline for future investigations using emovome, we implemented emotion recognition models using both speech and audio transcriptions. for speech, we used the standard egemaps feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. for text, we fine-tuned a multilingual bert model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively. this database will significantly contribute to research on emotion recognition in the wild, while also providing a unique natural and freely accessible resource for spanish.",2024-02-27
"130","ssresf: sensitivity-aware single-particle radiation effects simulation framework in soc platforms based on svm algorithm","meng liu (1), shuai li (1), fei xiao (1), ruijie wang (1), chunxue liu (2), liang wang (2) ((1) faculty of information technology, school of microelectronics, beijing university of technology, beijing, china, (2) beijing microelectronics technology institute, beijing, china)","hardware architecture","the ever-expanding scale of integrated circuits has brought about a significant rise in the design risks associated with radiation-resistant integrated circuit chips. traditional single-particle experimental methods, with their iterative design approach, are increasingly ill-suited for the challenges posed by large-scale integrated circuits. in response, this article introduces a novel sensitivity-aware single-particle radiation effects simulation framework tailored for system-on-chip platforms. based on svm algorithm we have implemented fast finding and classification of sensitive circuit nodes. additionally, the methodology automates soft error analysis across the entire software stack. the study includes practical experiments focusing on risc-v architecture, encompassing core components, buses, and memory systems. it culminates in the establishment of databases for single event upsets (seu) and single event transients (set), showcasing the practical efficacy of the proposed methodology in addressing radiation-induced challenges at the scale of contemporary integrated circuits. experimental results have shown up to 12.78x speed-up on the basis of achieving 94.58% accuracy.",2024-02-27
"131","enhancing eeg-to-text decoding through transferable representations from pre-trained contrastive eeg-text masked autoencoder","jiaqi wang, zhenxi song, zhengyu ma, xipeng qiu, min zhang, zhiguo zhang","computation and language","reconstructing natural language from non-invasive electroencephalography (eeg) holds great promise as a language decoding technology for brain-computer interfaces (bcis). however, eeg-based language decoding is still in its nascent stages, facing several technical issues such as: 1) absence of a hybrid strategy that can effectively integrate cross-modality (between eeg and text) self-learning with intra-modality self-reconstruction of eeg features or textual sequences; 2) under-utilization of large language models (llms) to enhance eeg-based language decoding. to address above issues, we propose the contrastive eeg-text masked autoencoder (cet-mae), a novel model that orchestrates compound self-supervised learning across and within eeg and text through a dedicated multi-stream encoder. furthermore, we develop a framework called e2t-ptr (eeg-to-text decoding using pretrained transferable representations), which leverages pre-trained modules alongside the eeg stream from cet-mae and further enables an llm (specifically bart) to decode text from eeg sequences. comprehensive experiments conducted on the popular text-evoked eeg database, zuco, demonstrate the superiority of e2t-ptr, which outperforms the state-of-the-art in rouge-1 f1 and bleu-4 scores by 8.34% and 32.21%, respectively. these results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread bci applications.",2024-02-27
"132","scalable identification of minimum undesignable rna motifs on loop-pair graphs","tianshuo zhou, wei yu tang, david h. mathews, liang huang","data structures and algorithms","motivation: rna design aims to find at least one sequence that folds with the highest probability into a designated target structure, but some structures are undesignable in the sense that no sequence folds into them. identifying undesignable structures is useful in delineating and understanding the limit of rna designability, but has received little attention until recently. in addition, existing methods on undesignability are not scalable and not interpretable. results: we introduce a novel graph representation and a new general algorithmic framework to efficiently identify undesignable motifs in a secondary structure. the proposed algorithm enumerates minimal motifs based on the loop-pair graph representation of a structure and establishes the undesignability of a motif by proposing rival substructure(s). our work can also identify unique minimum undesignable motifs across different structures. our implemented algorithms successfully identify 26 unique minimum undesignable motifs among 18 undesignable puzzles from the benchmark eterna100. additionally, our algorithm is so efficient that it scales to natural structures of 16s and 23s ribosomal rnas (about 1,500 and 3,000 nucleotides, resp.), and finds all of those structures in the widely used archiveii database to be undesignable, with 73 unique minimum undesignable motifs, under the standard turner energy model in viennarna.",2024-02-27
"133","nocplace: nocturnal visual place recognition using generative and inherited knowledge transfer","bingxi liu, yiqun wang, huaqi tao, tingjun huang, fulin tang, yihong wu, jinqiang cui, hong zhang","computer vision and pattern recognition","visual place recognition (vpr) is crucial in computer vision, aiming to retrieve database images similar to a query image from an extensive collection of known images. however, like many vision-related tasks, learning-based vpr often experiences a decline in performance during nighttime due to the scarcity of nighttime images. specifically, vpr needs to address the cross-domain problem of night-to-day rather than just the issue of a single nighttime domain. in response to these issues, we present nocplace, which leverages a generated large-scale, multi-view, nighttime vpr dataset to embed resilience against dazzling lights and extreme darkness in the learned global descriptor. firstly, we establish a day-night urban scene dataset called nightcities, capturing diverse nighttime scenarios and lighting variations across 60 cities globally. following this, an unpaired image-to-image translation network is trained on this dataset. using this trained translation network, we process an existing vpr dataset, thereby obtaining its nighttime version. the nocplace is then fine-tuned using night-style images, the original labels, and descriptors inherited from the daytime vpr model. comprehensive experiments on various nighttime vpr test sets reveal that nocplace considerably surpasses previous state-of-the-art methods.",2024-02-27
"134","metasql: a generate-then-rank framework for natural language to sql translation","yuankai fan, zhenying he, tonghui ren, can huang, yinan jing, kai zhang, x.sean wang","databases","the natural language interface to databases (nlidb) empowers non-technical users with database access through intuitive natural language (nl) interactions. advanced approaches, utilizing neural sequence-to-sequence models or large-scale language models, typically employ auto-regressive decoding to generate unique sql queries sequentially. while these translation models have greatly improved the overall translation accuracy, surpassing 70% on nlidb benchmarks, the use of auto-regressive decoding to generate single sql queries may result in sub-optimal outputs, potentially leading to erroneous translations. in this paper, we propose metasql, a unified generate-then-rank framework that can be flexibly incorporated with existing nlidbs to consistently improve their translation accuracy. metasql introduces query metadata to control the generation of better sql query candidates and uses learning-to-rank algorithms to retrieve globally optimized queries. specifically, metasql first breaks down the meaning of the given nl query into a set of possible query metadata, representing the basic concepts of the semantics. these metadata are then used as language constraints to steer the underlying translation model toward generating a set of candidate sql queries. finally, metasql ranks the candidates to identify the best matching one for the given nl query. extensive experiments are performed to study metasql on two public nlidb benchmarks. the results show that the performance of the translation models can be effectively improved using metasql.",2024-02-27
"135","swiftcache: model-based learning for dynamic content caching in cdns","bahman abolhassani, atilla eryilmaz, tom hou","optimization and control","we introduce swiftcache, a ""fresh"" learning-based caching framework designed for content distribution networks (cdns) featuring distributed front-end local caches and a dynamic back-end database. users prefer the most recent version of the dynamically updated content, while the local caches lack knowledge of item popularity and refresh rates. we first explore scenarios with requests arriving at a local cache following a poisson process, whereby we prove that the optimal policy features a threshold-based structure with updates occurring solely at request arrivals. leveraging these findings, swiftcache is proposed as a model-based learning framework for dynamic content caching. the simulation demonstrates near-optimal cost for poisson process arrivals and strong performance with limited cache sizes. for more general environments, we present a model-free reinforcement learning (rl) based caching policy without prior statistical assumptions. the model-based policy performs well compared to the model-free policy when the variance of interarrival times remains moderate. however, as the variance increases, rl slightly outperforms model-based learning at the cost of longer training times, and higher computational resource consumption. model-based learning's adaptability to environmental changes without retraining positions it as a practical choice for dynamic network environments. distributed edge caches can utilize this approach in a decentralized manner to effectively meet the evolving behaviors of users.",2024-02-27
"136","a fine-tuning enhanced rag system with quantized influence measure as ai judge","keshav rangan, yiqiao yin","information retrieval","this study presents an innovative enhancement to retrieval-augmented generation (rag) systems by seamlessly integrating fine-tuned large language models (llms) with vector databases. this integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced llms. central to our approach are the lora and qlora methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. a novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability. additionally, we introduce a quantized influence measure (qim) as an innovative ""ai judge"" mechanism to enhance the precision of result selection, further refining the system's accuracy. accompanied by an executive diagram and a detailed algorithm for fine-tuning qlora, our work provides a comprehensive framework for implementing these advancements within chatbot technologies. this research contributes significant insights into llm optimization for specific uses and heralds new directions for further development in retrieval-augmented models. through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational ai systems.",2024-02-26
"137","refactor: learning to extract theorems from proofs","jin peng zhou, yuhuai wu, qiyang li, roger grosse","artificial intelligence","human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. in this paper, we propose a novel method called theorem-from-proof extractor (refactor) for training neural networks to mimic this ability in formal mathematical theorem proving. we show on a set of unseen proofs, refactor is able to extract 19.6% of the theorems that humans would use to write the proofs. when applying the model to the existing metamath library, refactor extracted 16 new theorems. with newly extracted theorems, we show that the existing proofs in the metamath database can be refactored. the new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems. code can be found at this https url.",2024-02-26
"138","offline writer identification using convolutional neural network activation features","vincent christlein, david bernecker, andreas maier, elli angelopoulou","computer vision and pattern recognition","convolutional neural networks (cnns) have recently become the state-of-the-art tool for large-scale image classification. in this work we propose the use of activation features from cnns as local descriptors for writer identification. a global descriptor is then formed by means of gmm supervector encoding, which is further improved by normalization with the kl-kernel. we evaluate our method on two publicly available datasets: the icdar 2013 benchmark database and the cvl dataset. while we perform comparably to the state of the art on cvl, our proposed method yields about 0.21 absolute improvement in terms of map on the challenging bilingual icdar dataset.",2024-02-26
"139","the good and the bad: exploring privacy issues in retrieval-augmented generation (rag)","shenglai zeng, jiankun zhang, pengfei he, yue xing, yiding liu, han xu, jie ren, shuaiqiang wang, dawei yin, yi chang, jiliang tang","cryptography and security","retrieval-augmented generation (rag) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. whereas extensive research has demonstrated the privacy risks of large language models (llms), the rag technique could potentially reshape the inherent behaviors of llm generation, posing new privacy issues that are currently under-explored. in this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of rag systems on leaking the private retrieval database. despite the new risk brought by rag on the retrieval data, we further reveal that rag can mitigate the leakage of the llms' training data. overall, we provide new insights in this paper for privacy protection of retrieval-augmented llms, which benefit both llms and rag systems builders. our code is available at this https url.",2024-02-23
"140","using text embedding models and vector databases as text classifiers with the example of medical data","rishabh goel","information retrieval","the advent of large language models (llms) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. in tandem with llms, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. researchers at google have developed a clear alternative model, med-palm [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. when training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. here, we explore the use of vector databases and embedding models as a means of encoding, and classifying text with the example and application in the field of medicine. we show the robustness of these tools depends heavily on the sparsity of the data presented, and even with low amounts of data in the vector database itself, the vector database does a good job at classifying data [9]. using various llms to generate the medical data, we also understand the limitations of the medical knowledge of these models and encourage further expert medical review of our testing data. by using vector databases to classify a clinician's notes on a patient presented with a certain ailment, we understand the limitations of such methods, but also the promise of their prospective use and with continued testing and experimentation, hope to explore a unique use case of vector databases and embedding models.",2024-02-07
"141","oncogpt: a medical conversational model tailored with oncology domain expertise on a large language model meta-ai (llama)","fujian jia, xin liu, lixi deng, jiwen gu, chunchao pu, tunan bai, mengjiang huang, yuanzhi lu, kang liu","computation and language","in the past year, there has been a growing trend in applying large language models (llms) to the field of medicine, particularly with the advent of advanced language models such as chatgpt developed by openai. however, there is limited research on llms specifically addressing oncology-related queries. the primary aim of this research was to develop a specialized language model that demonstrates improved accuracy in providing advice related to oncology. we performed an extensive data collection of online question-answer interactions centered around oncology, sourced from reputable doctor-patient platforms. following data cleaning and anonymization, a dataset comprising over 180k+ oncology-related conversations was established. the conversations were categorized and meticulously reviewed by field specialists and clinicians to ensure precision. employing the llama model and other selected open-source datasets, we conducted iterative fine-tuning to enhance the model's proficiency in basic medical conversation and specialized oncology knowledge. we observed a substantial enhancement in the model's understanding of genuine patient inquiries and its reliability in offering oncology-related advice through the utilization of real online question-answer interactions in the fine-tuning process. we release database and models to the research community (this https url).",2024-02-26
"142","structlm: towards building generalist models for structured knowledge grounding","alex zhuang, ge zhang, tianyu zheng, xinrun du, junjie wang, weiming ren, stephen w. huang, jie fu, xiang yue, wenhu chen","computation and language","structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. despite the demonstrated capabilities of large language models (llms) on plain text, their proficiency in interpreting and utilizing structured data remains limited. our investigation reveals a notable deficiency in llms' ability to process structured data, e.g., chatgpt lags behind state-of-the-art (sota) model by an average of 35%. to augment the structured knowledge grounding (skg) capabilities in llms, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. utilizing this dataset, we train a series of models, referred to as structlm, based on the code-llama architecture, ranging from 7b to 34b parameters. our structlm series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new sota achievements on 7 skg tasks. furthermore, structlm demonstrates exceptional generalization across 6 novel skg tasks. contrary to expectations, we observe that scaling model size offers marginal benefits, with structlm-34b showing only slight improvements over structlm-7b. this suggests that structured knowledge grounding is still a challenging task and requires more innovative design to push to a new level.",2024-02-26
"143","boxrec: recommending a box of preferred outfits in online shopping","debopriyo banerjee, krothapalli sreenivasa rao, shamik sural, niloy ganguly","information retrieval","over the past few years, automation of outfit composition has gained much attention from the research community. most of the existing outfit recommendation systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by recommendation of top-n outfits or a capsule wardrobe having a collection of outfits based on user's fashion taste. however, none of these consider user's preference of price-range for individual clothing types or an overall shopping budget for a set of items. in this paper, we propose a box recommendation framework - boxrec - which at first, collects user preferences across different item types (namely, top-wear, bottom-wear and foot-wear) including price-range of each type and a maximum shopping budget for a particular shopping session. it then generates a set of preferred outfits by retrieving all types of preferred items from the database (according to user specified preferences including price-ranges), creates all possible combinations of three preferred items (belonging to distinct item types) and verifies each combination using an outfit scoring framework - boxrec-osf. finally, it provides a box full of fashion items, such that different combinations of the items maximize the number of outfits suitable for an occasion while satisfying maximum shopping budget. empirical results show superior performance of boxrec-osf over the baseline methods.",2024-02-26
"144","aligning large language models to a domain-specific graph database","yuanyuan liang, keren tan, tingyu xie, wenbiao tao, siyuan wang, yunshi lan, weining qian","computation and language","graph databases (graph db) are widely applied in various fields, including finance, social networks, and medicine. however, translating natural language (nl) into the graph query language (gql), commonly known as nl2gql, proves to be challenging due to its inherent complexity and specialized nature. some approaches have sought to utilize large language models (llms) to address analogous tasks like text2sql. nevertheless, when it comes to nl2gql taskson a particular domain, the absence of domain-specific nl-gql data pairs makes it difficult to establish alignment between llms and the graph db. to address this challenge, we propose a well-defined pipeline. specifically, we utilize chatgpt to create nl-gql data pairs based on the given graph db with self-instruct. then, we use the created data to fine-tune llms, thereby achieving alignment between llms and the graph db. additionally, during inference, we propose a method that extracts relevant schema to the queried nl as the input context to guide llms for generating accurate gqls.we evaluate our method on two constructed datasets deriving from graph dbs in finance domain and medicine domain, namely fingql and medigql. experimental results demonstrate that our method significantly outperforms a set of baseline methods, with improvements of 5.90 and 6.36 absolute points on em, and 6.00 and 7.09 absolute points on ex, respectively.",2024-02-26
"145","retrouver l'inventeur-auteur : la lev{é}e d'homonymies d'autorat entre les brevets et les publications scientifiques","david reymond (imsic), heman khouilla (lead), sandrine wolff (beta), manuel durand-barthez (cjm, urfist paris)","information retrieval","patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes. authors' and inventors' names are the key identifiers to carry out these analyses, which however, run up against the issue of disambiguation. by extension identifying inventors who are also academic authors is a non-trivial challenge. we propose a method using the international patent classification (ipc) and the ipccat api to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents. the method is developed and manually qualified based on three corpora of patents extracted from the international epo database espacenet. among a set of 4679 patents and 7720 inventors, we obtain 2501 authors. the proposed algorithm solves the general problem of disambiguation with an error rate lower than 5%.",2024-02-26
"146","cyclic sieving on permutations -- an analysis of maps and statistics in the findstat database","ashleigh adams, jennifer elder, nadia lafrenière, erin mcnicholas, jessica striker, amanda welch","combinatorics","we perform a systematic study of permutation statistics and bijective maps on permutations using sagemath to search the findstat combinatorial statistics database to identify apparent instances of the cyclic sieving phenomenon (csp). cyclic sieving occurs on a set of objects, a statistic, and a map of order $n$ when the evaluation of the statistic generating function at the $d$th power of the primitive $n$th root of unity equals the number of fixed points under the $d$th power of the map. of the apparent instances found in our experiment, we prove 34 new instances of the csp, and conjecture three more. furthermore, we prove the equidistribution of some statistics and show that some maps have the same orbit structure, thus cyclic sieving holds for more even more pairs of a map and a statistic. the maps which exhibit the csp include reverse/complement, rotation, lehmer code rotation, toric promotion, and conjugation by the long cycle, as well as a map constructed by corteel to swap the number of nestings and crossings, the invert laguerre heap map, and a map of alexandersson and kebede designed to preserve right-to-left minima. our results show that, contrary to common expectations, actions that exhibit homomesy are not necessarily the best candidates for the csp, and vice versa.",2024-02-26
"147","combining machine learning with computational fluid dynamics using openfoam and smartsim","tomislav maric, mohammed elwardi fadeli, alessandro rigazzi, andrew shao, andre weiner","machine learning","combining machine learning (ml) with computational fluid dynamics (cfd) opens many possibilities for improving simulations of technical and natural systems. however, cfd+ml algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging. we provide an effective and scalable solution to developing cfd+ml algorithms using open source software openfoam and smartsim. smartsim provides an orchestrator that significantly simplifies the programming of cfd+ml algorithms and a redis database that ensures highly scalable data exchange between ml and cfd clients. we show how to leverage smartsim to effectively couple different segments of openfoam with ml, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. we additionally provide an openfoam sub-module with examples that can be used as starting points for real-world applications in cfd+ml.",2024-02-25
"148","differentially private bayesian persuasion","yuqi pan, zhiwei steven wu, haifeng xu, shuran zheng","computer science and game theory","the tension between persuasion and privacy preservation is common in real-world settings. online platforms should protect the privacy of web users whose data they collect, even as they seek to disclose information about these data to selling advertising spaces. similarly, hospitals may share patient data to attract research investments with the obligation to preserve patients' privacy. to deal with these issues, we develop a framework to study bayesian persuasion under differential privacy constraints, where the sender must design an optimal signaling scheme for persuasion while guaranteeing the privacy of each agent's private information in the database. to understand how privacy constraints affect information disclosure, we explore two perspectives within bayesian persuasion: one views the mechanism as releasing a posterior about the private data, while the other views it as sending an action recommendation. the posterior-based formulation helps consider privacy-utility tradeoffs, quantifying how the tightness of privacy constraints impacts the sender's optimal utility. for any instance in a common utility function family and a wide range of privacy levels, a significant constant utility gap can be found between any two of the three conditions: $\epsilon$-differential privacy constraint, relaxation $(\epsilon,\delta)$-differential privacy constraint, and no privacy constraint. we further geometrically characterize optimal signaling schemes under different types of constraints ($\epsilon$-differential privacy, $(\epsilon,\delta)$-differential privacy and renyi differential privacy), all of which can be seen as finding concave hulls in constrained posterior regions. meanwhile, by taking the action-based view of persuasion, we provide polynomial-time algorithms for computing optimal differentially private signaling schemes, as long as a mild homogeneous condition is met.",2024-02-24
"149","implications of self-identified race, ethnicity, and genetic ancestry on genetic association studies in biobanks within health systems","ruth johnson, bogdan pasaniuc","other quantitative biology","precision medicine aims to create biomedical solutions tailored to specific factors that affect disease risk and treatment responses within the population. the success of the genomics era and recent widespread availability of electronic health records (ehr) has ushered in a new wave of genomic biobanks connected to ehr databases (ehr-linked biobanks). this perspective aims to discuss how race, ethnicity, and genetic ancestry are currently utilized to study common disease variation through genetic association studies. although genetic ancestry plays a significant role in shaping the genetic landscape underlying disease risk in humans, the overall risk of a disease is caused by a complex combination of environmental, sociocultural, and genetic factors. when using ehr-linked biobanks to interrogate underlying disease etiology, it is also important to be aware of how the biases associated with commonly used descent-associated concepts such as race and ethnicity can propagate to downstream analyses. we intend for this resource to support researchers who perform or analyze genetic association studies in the ehr-linked biobank setting such as those involved in consortium-wide biobanking efforts. we provide background on how race, ethnicity, and genetic ancestry play a role in current association studies, highlight considerations where there is no consensus about best practices, and provide transparency about the current shortcomings.",2024-02-24
"150","general predictions of neutron star properties using unified relativistic mean-field equations of state","luigi scurto, helena pais, francesca gulminelli","nuclear theory","in this work we present general predictions for the static observables of neutron stars (nss) under the hypothesis of a purely nucleonic composition of the ultra-dense baryonic matter, using bayesian inference on a very large parameter space conditioned by both astrophysical and nuclear physics constraints. the equation of states are obtained using a unified approach of the ns core and inner crust within a fully covariant treatment based on a relativistic mean-field lagrangian density with density dependent couplings. the posterior distributions are well compatible with the ones obtained by semi-agnostic meta-modelling techniques based on non-relativistic functionals, that span a similar portion of the parameter space in terms of nuclear matter parameters, and we confirm that the hypothesis of a purely nucleonic composition is compatible with all the present observations. we additionally show that present observations do not exclude the existence of very massive neutron stars with mass compatible with the lighter partner of the gravitational event gw190814 measured by the ligo-virgo collaboration. some selected representative models, that respect well all the constraints taken into account in this study, and approximately cover the residual uncertainty in our posterior distributions, will be uploaded in the compose database for use by the community.",2024-02-23
